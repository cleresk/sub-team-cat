In this chapter we go over the methodology details of our experiment. We are interested in comparing the gaze behavior of the participants when they are presented with static images or moving images accompanied by a related audible sentence.
This was done by collecting gaze data and analyzing the gaze behavior of the participants (@sec-analysis). 

## Participants

We had 10 people participating in our experiment between the ages 18 and 34 where nine people identified as male and one as female.Regarding their English level, most (6) people specified that they have reached at least a C1 level and the remaining (4) people were at a B2 level. Additionally, we asked the participants about their previous experience with eye tracking. For the majority (7) it was the first time using eye trackers, the other three stated that they have used eye trackers before.

Overall, all the participants were able to complete the experiment successfully within 10 to 15 minutes after the calibration. 


## Study Design
We have a between-subject study design with one factor, which is the visual stimuli type. The factor has two possible levels, with one being static and the other in motion. 
The participants are exposed to all 64 audio stimuli, but whether the corresponding visual stimuli is static or in motion depends on their condition group which was determined by their randomly assigned participant ID. For instance, if for stimuli nr. 5 the visual was static for condition group “Chicken”, the stimuli type was simply flipped for the other condition group “Wolf”. With this latin square design, we made sure that the stimuli/condition mappings were equally distributed among both condition groups, as long as the amount of participants in the groups were the same. The order of the stimuli overall was random for each participant by utilizing the random order function within OpenSesame. In summary, we have 64 audio stimuli with two possible matching visual stimuli. The participants were shown 32 visual stimuli for each category, which was determined by their randomly assigned condition group. 


## Materials & Stimuli
We have 64 audio stimuli in total with two matching visual stimuli versions, one consisting of four regular images and the other with four GIFs. 
The audio stimuli consisted of different sentences with the same structure: [location/time], [person] [verb] [object], e.g.: “At breakfast, the girl ate a croissant.”
The corresponding visual stimuli consists of four images with only one being closely related to the sentence, in the case of our example we have one image being a croissant and the remaining images being random inedible objects that are not related to breakfast. For the motion stimuli, we just picked out matching GIFs for the four static images.  
We came up with the 64 different sentences by following the described structure and created corresponding audios with a free text-to-speech tool (https://www.acoust.io/).
For the visual stimuli we re-used most of the images from the visual world paradigm tutorial by OpenSesame and looked for additional ones (including matching moving ones) through google images. 

The stimuli creation process turned out to be a cumbersome process with handling the text-to-speech tool to include breaks and not speak too fast, finding visual stimuli to go with the audios and making sure that there is only one matching target sub-stimuli for each sentence. With our amount of stimuli, it is easy to lose sight of all the different sub-images. Therefore, to make the stimuli creation process easier, we used a central documentation (excel-table) where all the sentences and their corresponding sub-stimuli file names were stated, and the mapping of the shown stimuli version for each condition group. In case something changed, we only had to change the central document and paste its contents to our trial loop in OpenSesame. 
In the following we provide some stimuli examples: 
(2 static stimuli, 2 motion stimuli) 1 big image? (figma)

## Implementation
@Karl

## Study Procedure and Apparatus
To test our hypothesis, we needed to invite participants to partake in our experiment. <br>
We recruited participants by asking for volunteers in our circle of acquaintances and invited the interested people between mid-July and August. Our target sample size was at least ten participants to end up with at least five people per condition group.
Once a participant arrived, they were given an information sheet with a brief description of the experiment, what kind of devices are going to get used and their corresponding safety risks. Additionally, they were asked to sign a standard consent form for studies with humans that are conducted at the university. 
To counterbalance the number of participants per condition group, we made them pick a card with a unique number between 1-10 and depending on whether the ID was an odd number or not, they got assigned to the corresponding condition group. 
The participants were asked to make themselves comfortable on the chair and adjust it accordingly to their preference, the same goes for the standard chin rest we used. Both the chair and the chin rest were disinfected before each participant arrived.
To execute the experiment, we used a standard desktop PC with two displays standing back-to-back in duplicate view, where one was directed to the participant and the other was used by the study conductor to start and supervise the experiment. 
For tracking, we used the stationary GazePoint GP3 HD eye-tracker (TODO: what frequency to track??)
When starting the experiment in OpenSesame, it starts out with initializing the eye-tracker and calibrating. The calibration process sometimes took multiple attempts until the results looked precise and accurate enough. We always told the participants that the calibration process can require multiple attempts to make sure they dont feel like they are doing something wrong. 
After the setup, the experiment itself lasted around 12 to 15 minutes. The participants had the control to proceed to the next trial in their own speed and they were informed, that they can take breaks in-between trials, if needed. 
Finally, we asked the participants some basic demographic information and their previous experiences with eye tracking. We placed the demographic survey at the end of the experiment to avoid stereotype threat (see @spencer2016stereotype).