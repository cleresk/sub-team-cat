{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Investigating the Visual World Paradigm\"\n",
        "subtitle: \"Do people still make anticipatory eye movements when presented images with motion instead of static ones?\"\n",
        "\n",
        "number-sections: true\n",
        "date: today\n",
        "date-format: long\n",
        "\n",
        "title-block-banner: true\n",
        "title-block-style: default\n",
        "\n",
        "bibliography: bibliography.bib\n",
        "\n",
        "author: \n",
        "- name: Elif Dilara Aygün\n",
        "- name: Karl Jorge Cleres Andreo\n",
        "- name: Yanhong Xu\n",
        "\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    code-fold: true\n",
        "    number-sections: true\n",
        "    colorlinks: true\n",
        "    code-block-bg: true\n",
        "    code-block-border-left: \"#31BAE9\"\n",
        "  pdf:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    colorlinks: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Introduction {#sec-introduction}\n",
        "\n",
        "The Visual World Paradigm (VWP) is an established method in psycholinguistics that involves tracking eye movements to study how individuals process spoken language while viewing visual scenes. It allows researchers to understand how linguistic information influences attention and gaze behavior in real-time. As part of the course \"Acquisition and Analysis of Eye-Tracking Data\" we implemented the VWP in an experiment with a subsequent data analysis. In this report we provide details to allow reproducibility of the experiment, discuss our study design choices and present our results.\n",
        "\n",
        "## Background\n",
        "The role of language and visual processing has been studied by many researchers using eye tracking in the visual world paradigm (VWP). VWP is a useful eye-tracking technique which consists of visual scenes that a participant views while listening to some spoken utterances. Participants look at the visual display which contains pictures of objects, meanwhile, the eye tracker monitors the eye movements as the heard language unfolds over time. The spoken utterance can extend from a single word to short narrative or even a story, and the utterance can be related to more than one object. When the eye movement of a saccade (a quick change of gaze moves from one point of fixation to another) happens, the eye tracker yields a time estimation at which word and corresponding picture has been identified on the visual scene while listening to the utterance. In this respect, VWP examines the relationship between the visual stimuli and language input, which also offers a different insight of exploring the integration of visual and language processing happening in the brain (Huettig at al, 2011 & Berends et al, 2016).\n",
        "\n",
        "How people interpret and process spoken language in the context of their contemporary visual field and how eye movements are involved as the language is being processed were first demonstrated experimentally by Cooper (1974). However, the vast majority of researchers have not considered the effects of Cooper’s study, and there is a relatively small body of literature that is concerned with the language processing and eye movements. \n",
        "\n",
        "After a few decades, since Tanenhaus et al. (1995) introduced the influential study of VWP in real-time spoken language comprehension, the rapid development of VWP started to flourish in the field, additionally with the contemporary high quality eye-trackers, VWP also has a transformative influence in psycholinguistics (Clifton et al, 2007). \n",
        "\n",
        "\n",
        "\n",
        " A key study by Altmann and Kamide (1999) demonstrated that anticipatory eye movements could be predicted based on the verb used in a sentence. For example, upon hearing a verb like \"eat,\" participants' gaze would quickly shift to an edible object in the scene before the object itself was mentioned, indicating that verb meaning constrains visual attention.\n",
        "\n",
        "This ability to predict upcoming referents based on verb semantics highlights the close interaction between language processing and visual attention, providing crucial insights into how language guides our interpretation of the visual world. The original Altmann & Kamide (1999) study used static images, focusing on how specific verbs direct gaze towards the most semantically appropriate object among a set of alternatives.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "We want to build on top of the foundational work of Altmann & Kamide (1999), by aiming to explore how dynamic visual stimuli might affect anticipatory eye movements within the VWP. Given that motion is a powerful cue in guiding visual attention, this study introduces video stimuli to examine whether the presence of motion has an effect on the predictive gaze patterns observed with static images.\n",
        "\n",
        "The introduction of motion provides a new dimension to understanding the interaction between linguistic information and visual attention. While previous studies have shown that static images paired with predictive verbs can direct gaze, it remains unclear how motion in the visual scene impacts this process. \n",
        "\n",
        "## Research Question / Hypothesis\n",
        "Building upon the results of Altmann & Kamide (1999), which suggested that sentence processing is driven by predictive relationships between verbs and objects, our study aims to extend this understanding by introducing moving stimuli into the experimental setup. Specifically, we propose our research question:\n",
        "\n",
        "\n",
        "<p style=\"text-align: center;\">*Do people still make anticipatory eye movements when presented with images that are in motion instead of static ones?*</p>\n",
        "This question is grounded in the idea that while humans are naturally inclined to focus on moving objects (TODO: Find References here), the extent to which this influences the predictive power of verbs during sentence processing remains unknown. We hypothesize that the addition of motion might modulate the anticipatory eye movements either by introducing competing visual stimuli that might disrupt the prediction.\n",
        "\n",
        "\n",
        "\n",
        "# Experiment {#sec-experiment}\n",
        "\n",
        "\n",
        "In this chapter we go over the methodology details of our experiment. We are interested in comparing the gaze behavior of the participants when they are presented with static or moving images accompanied by a related audible sentence.\n",
        "This was done by collecting gaze data and analyzing the gaze behavior of the participants (@sec-analysis). \n",
        "\n",
        "## Participants\n",
        "\n",
        "We had 10 people participating in our experiment between the ages 18 and 34 where nine people identified as male and one as female. Regarding their English level, most (6) people specified that they have reached at least a C1 level and the remaining (4) people were at a B2 level. Additionally, we asked the participants about their previous experience with eye-tracking. For the majority (7) it was the first time using eye-trackers, the other three stated that they have used eye-trackers before.\n",
        "\n",
        "Overall, all the participants were able to complete the experiment successfully without significant complications.\n",
        "\n",
        "\n",
        "## Study Design\n",
        "We have a between-subject study design with one factor being the visual stimuli type. The factor has two possible levels, with one being static and the other in motion. \n",
        "The participants are exposed to all 64 audio stimuli, but whether the corresponding visual stimuli is static or in motion depends on their condition group, which was determined by their randomly assigned participant ID. For instance, if for stimuli nr. 5 the visual was static for condition group “Chicken”, the stimuli type was simply flipped for the other condition group “Wolf”. With this latin square design, we made sure that the stimuli/condition mappings were equally distributed among both condition groups, as long as the amount of participants in the groups were the same. The order of the stimuli overall was random for each participant by utilizing the random order function within OpenSesame. In summary, we have 64 audio stimuli with two possible matching visual stimuli. The participants were shown 32 visual stimuli for each category, which was determined by their randomly assigned condition group. \n",
        "Each trial started with a fixation dot for one second, followed by a preview of the four images for another second. Then, the audio started playing and the images were still shown 500 miliseconds after audio completion, after which the experiment switched back to the fixation dot and the trial was completed. The participant is then able to proceed to the next trial by looking at the fixation dot. A visualization of the trial sequence is shown in figure ![Figure 2.2.1](Images/Fig2_2_1.png){width=55%,fig-align=\"center\"}.\n",
        "\n",
        "## Materials & Stimuli\n",
        "We have 64 audio stimuli in total with two matching visual stimuli versions, one consisting of four regular images and the other with four GIFs. \n",
        "The audio stimuli consisted of different sentences with the same structure: [location/time], [person] [verb] [object], e.g.: “At breakfast, the girl ate a croissant.”\n",
        "The corresponding visual stimuli consists of four images with only one being closely related to the sentence. In the case of our example we have one image being a croissant and the remaining images being random inedible objects that are not related to breakfast. For the motion stimuli, we just picked out matching GIFs for the four static images. It was important, that the gives had the same speed. The exact specifications for static and motion stimuli as well as the audio stimuli can be found in the README-File of the experiment.  \n",
        "We came up with the 64 different sentences by following the described structure and created corresponding audios with a free text-to-speech tool[^1].\n",
        "For the visual stimuli we re-used most of the images from the visual world paradigm tutorial by OpenSesame [^2] and looked for additional ones using Google images. \n",
        "\n",
        "The stimuli creation process turned out to be a cumbersome process with handling the text-to-speech tool to include breaks and not speak too fast, finding visual stimuli to go with the audios and making sure that there is only one matching target sub-stimuli for each sentence. With our amount of stimuli, it is easy to lose sight of all the different sub-images. Therefore, to make the stimuli creation process easier, we used a central excel sheet as documentation where all the sentences and their corresponding sub-stimuli file names were stated, including which stimuli version (static or motion) will be shown for each condition group. In case something changed, we only had to change the central document and paste its contents to our trial loop in OpenSesame. \n",
        "In the following we provide some stimuli examples: \n",
        "![Figure 2.3.1](Images/Fig2_3_1.png)\n",
        "\n",
        "[^1]: https://www.acoust.io/\n",
        "\n",
        "[^2]: https://osdoc.cogsci.nl/3.3/tutorials/visual-world/\n",
        "\n",
        "\n",
        "\n",
        "## Study Procedure and Apparatus {#sec-stdy-proc}\n",
        "To test our hypothesis, we needed to invite participants to partake in our experiment. <br>\n",
        "We recruited participants by asking for volunteers in our circle of acquaintances and invited the interested people between mid-July and August. Our target sample size was at least ten participants to end up with at least five people per condition group.\n",
        "Once a participant arrived, they were given an information sheet with a brief description of the experiment, what kind of devices are going to get used and their corresponding safety risks. Additionally, they were asked to sign a standard consent form for studies with humans that are conducted at the university. \n",
        "To counterbalance the number of participants per condition group, we made them pick a card with a unique number between 1-10 and depending on whether the ID was an odd number or not, they got assigned to the corresponding condition group. \n",
        "The participants were asked to make themselves comfortable on the chair and adjust it accordingly to their preference, the same goes for the standard chin rest we used. Both the chair and the chin rest were disinfected before each participant arrived.\n",
        "To execute the experiment, we used a standard desktop PC with two displays standing back-to-back in duplicate view, where one was directed to the participant and the other was used by the study conductor to start and supervise the experiment. \n",
        "For tracking, we used the stationary GazePoint GP3 HD eye-tracker in 150 Hz mode. \n",
        "When starting the experiment in OpenSesame, it starts out with initializing the eye-tracker and calibrating. The calibration process sometimes took multiple attempts until the results looked precise and accurate enough. We always told the participants that the calibration process can require multiple attempts to make sure they dont feel like they are doing something wrong. \n",
        "After the setup, the experiment itself lasted around 12 to 15 minutes. The participants had the control to proceed to the next trial in their own speed and they were informed, that they can take breaks in-between trials, if needed. \n",
        "Finally, we asked the participants some basic demographic information and their previous experiences with eye-tracking. We placed the demographic survey at the end of the experiment to avoid stereotype threat (see @spencer2016stereotype).\n",
        "\n",
        "## Implementation\n",
        "\n",
        "The experiment was conducted using the most recent version of OpenSesame 4.0 Melodramatic Milgram with the PyGame backend. The logic is divided into three distinct logical units. <br>\n",
        "In the first unit is used to load custom configuration and preprocess static and motion stimuli. The config-file was managed with the YAML parser and emitter PyYAML [^3] and used to customize texts, their layouts and positions, fixation dots, positions of stimuli as well as the general timing of the expermient. This way we kept the experiment flexible for minor changes during the pilot studies or possible future research. Since it was not possible to display videos or gifs in OpenSesame, we used the Python library OpenCV [^4] to extract the frames of each mp4-Videofile and save them into a separate folder for each stimulus inside a temporary directory. This directory can be specified in the custom config File, stated above. <br>\n",
        "In the second unit of the experiment, the built-in calibration of OpenSesame was used to calibrate the eye-tracker and the group selection was displayed to the user. <br>\n",
        "The third unit consists of the trial loop. For each trial, the user is first shown a fixation dot, then the four substimuli and afterwards the sentence to the pictures is played as stated in @sec-stdy-proc. The trial was designed in such a way that it would only commence once the user had fixated on the central fixation dot. In order to maintain a consistent framerate for each motion stimulus, all the images were placed into an infinite loop, with each sub-stimulus containing an individual frame counter. This counter was reset to the beginning once it reached the final frame. Any concerns regarding the performance of the expriment could be cleared after extensive piloting. Once the designated sentence was played, the infinite loop was interrupted and the pause screen was displayed. The logging took place multiple times during the trial loop and finally logged with the OpenSesame logger. The logging will be discussed in @sec-logging.\n",
        "\n",
        "[^3]: https://pypi.org/project/PyYAML/\n",
        "[^4]: https://pypi.org/project/opencv-python/\n",
        "\n",
        "\n",
        "\n",
        "# Analysis Methods {#sec-analysis}\n",
        "@Karl\n",
        "\n",
        "This chapter shows how data was gathered, preprocessed and analyzed in order to answer the research question. The analysis was performed in Python 3.12.4 [^5] with the use of the libraries Pandas [^6] and NumPy [^7] for data manipulation and analysis as well as Matplotlib [^8] and Seaborn [^9] for visualisations. The resulting scripts are Jupyter Notebooks and a Python script that depending on the filename can be run for a single subject _(e.g. analysis-subjects.ipynb)_ or for all the subjects _(e.g. analysis-global.ipynb)_. \n",
        "\n",
        "[^5]: https://www.python.org/downloads/release/python-3124/\n",
        "[^6]: https://pypi.org/project/pandas/\n",
        "[^7]: https://pypi.org/project/numpy/\n",
        "[^8]: https://pypi.org/project/matplotlib/\n",
        "[^9]: https://pypi.org/project/seaborn/\n",
        "\n",
        "## Gathering data / logging {#sec-logging}\n",
        "\n",
        "In order to calibrate, record and log the gaze data in OpenSesame, we used several built-in pygaze elements. I.e. the the pygaze_init element befor the trial loop in order to perform the calibration. Pygaze_start_recording at the beginning of a trial followed by a logger and a pygaze_stop_recording at the end of a trial. <br>\n",
        "\n",
        "It is essential to record the occurence of specific events during the experiment, as these will be subject to subsequent analysis. This is achieved by custom logging with the logger item in OpenSesame. The structure of a custom log is always consistent. It comprises a string containing all the information from the current row in the stimuli table. The string is formated so that each cell of the row is logged as key-value pair and divided by a semicolon. \n",
        "\n",
        "> VAR TRIAL_LOG VERB_CUE=EAT;GROUP=CHICKEN;SENTENCE_ID=11; SENTENCE=AT THE FARM, THE GIRL FED THE CHICKEN. ; (...)\n",
        "\n",
        "In order to facilitate the differentiation of the current event, a key-value pair is appended to the logstring. Four potential events are logged within the experiment, as follows: \n",
        "\n",
        "* Fixation: This event is logged at the beginning of a trial, when the participant is presented the fixation dot on the screen.\n",
        "* Preview: Stimuli are presented. \n",
        "* Audiostart: The audio with the sentence starts playing. \n",
        "* Pause: The beginning of the pause, where the user has to look at the fixation dot in oder to continue.\n",
        "\n",
        "The subsequent analysis contains two more events which could not be logged in OpenSesame due to its design. Our solution to this problem will be presented in the next chapter.   \n",
        "\n",
        "## Preprocessing and quality control\n",
        "<!-- hier vllt noch sagen, dass das preprocessing jeweils nur für ein subject möglich ist -->\n",
        "At the beginning, we filtered out the data that was irrelevant for this research. The following code shows the columns that were dropped:"
      ],
      "id": "f063a980"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-ex\n",
        "#| fig-cap: Example code\n",
        "#| eval: false\n",
        "#| code-fold: false\n",
        "\n",
        "# (...) read data as pandas.DataFrame\n",
        "\n",
        "# drop unnecessary data\n",
        "data = data.drop([\"TIME_TICK\"], axis=1)\n",
        "data = data.drop([\"FPOGX\", \"FPOGY\", \"FPOGS\", \"FPOGD\", \"FPOGID\", \"FPOGV\"], axis=1) \n",
        "data = data.drop([\"LPOGX\", \"LPOGY\", \"LPOGV\", \"RPOGX\", \"RPOGY\", \"RPOGV\"], axis=1)\n",
        "data = data.drop([\"LPCX\", \"LPCY\", \"LPD\", \"LPS\", \"LPV\", \"RPCX\", \"RPCY\", \"RPD\", \"RPS\", \"RPV\"], axis=1)\n",
        "data = data.drop([\"LEYEX\", \"LEYEY\", \"LEYEZ\", \"LPUPILD\", \"LPUPILV\", \"REYEX\", \"REYEY\", \"REYEZ\", \"RPUPILD\", \"RPUPILV\"], axis=1)\n",
        "data = data.drop([\"CX\", \"CY\", \"CS\"], axis=1)"
      ],
      "id": "fig-ex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting Dataframe looks like this:\n",
        "\n",
        "| CNT | TIME | BPOGX | BPOGY | BPOGV | USER |\n",
        "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
        "| 30093 | 204.69383 | 0.33433 | 0.57871 | 1 | START_TRIAL 0 | \n",
        "\n",
        "Resulting data and its interpretation can be found in the GazePoint API documentation [^10] and forms the basis for the analysis. <br>\n",
        "\n",
        "At the beginning several sanity checks were performed in order to evaluate the quality of the collected data. The first one being the examination on recorded logs. Therefor the occurencies of all existing logs in the USER column were counted. Since the experiment has 64 trials there should be 64 hits for each log message except the *TRIAL_WARNING* that is only logged when the subject does not fixate on the fixation dot during a pause for more than 15 seconds. Afterwards the results are plotted in a barplot<br>\n",
        "\n",
        "By plotting a histogram of the time delta between samples, we found out, that it is not consistent. Therefor we interpolated the values of BPOGX, BPOGY, and BPOGV with the scipy [^11] linear (one dimensional) interpolator. The time was reorganized so that it matches the sampling rate. Since the eye tracker has a sampling rate of 150hz, the total time of the experiment in seconds was multiplyed by 150 to figure out the amount of bins for the interpolation function. The respective values for BPOGX, BPOGY and BPOGY at the location of each bin determined the interpolated value. In order to obtain a valid BPOGV, its value after interpolation was rounded to an integer value. <br>\n",
        "\n",
        "Figure @fig-interpolation shows an example function before and after interpolation:\n",
        "![Figure 3.2.1](Images/Fig3_2_1.png){#fig-interpolation fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "[^10]: https://www.gazept.com/dl/Gazepoint_API_v2.0.pdf\n",
        "[^11]: https://pypi.org/project/scipy/\n",
        "\n",
        "## Analysis pipeline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Results {#sec-results}\n",
        "\n",
        "\n",
        "TODO\n",
        "\n",
        "\n",
        "\n",
        "# Discussion {#sec-discussion}\n",
        "\n",
        "\n",
        "Discussion\n",
        "\n",
        "1- paragraph Summary\n",
        "\n",
        "How do the findings relate to the literature?\n",
        "\n",
        "Limitations\n",
        "\n",
        "Unlike the previously referenced work, we did not conduct the study with native speakers of the English language. \n",
        "\n",
        "Challenges during project\n",
        "\n",
        "## Outlook/ improvements / Lessons learned\n",
        "\n",
        "We observed some of our participants getting bored during the experiment, either they told us or they started fidgeting more during the trials seeming impatient. This indicates that we should plan a short break halfway through the trials, since the participants might not feel the need to speak up about needing a short break. \n",
        "\n",
        "\n",
        "\n",
        "# Contribution Table\n",
        "\n",
        "\n",
        "The following table gives a detailed insight on how the workload of this project was split between the group members:\n",
        "\n",
        "| Task                          | Yanhong|  Dilara   |Karl Jorge|\n",
        "|-------------------------------|--------|-----------|----------|\n",
        "|   Background Literature       |  x     |    o      |          |\n",
        "|   Experiment Design           |        |    x      |          |\n",
        "|   Experiment Implementation   |        |           |    x     |\n",
        "|   Stimulus and audio design   |  x     |           |          |\n",
        "|   Piloting                    |  x     |    x      |    x     |\n",
        "|   Pilot plots                 |        |    x      |    x     |\n",
        "|   Data-Recording              |  o     |    x      |    x     |\n",
        "|   Data Analysis Scripts       |        |           |    x     |\n",
        "|   Report Writing              |  x     |    x      |    o     |\n",
        "|   Report Lectorate            |  x     |           |          |\n",
        "|   Participant Acquisition     |  x     |     x     |    x     |\n",
        "|   Project Management          |        |    x      |          |\n",
        "\n",
        "Legend: x = main, o = supporter\n"
      ],
      "id": "f63a384d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\CleresAndreK\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}