The Visual World Paradigm (VWP) is an established method in psycholinguistics that involves tracking eye movements to study how individuals process spoken language while viewing visual scenes. It allows researchers to understand how linguistic information influences attention and gaze behavior in real-time. As part of the course "Acquisition and Analysis of Eye-Tracking Data" we implemented the VWP in an experiment with a subsequent data analysis. In this report we provide details to allow reproducibility of the experiment, discuss our study design choices and present our results.

## Background
The role of language and visual processing has been studied by many researchers using eye tracking in the visual world paradigm (VWP). VWP is a useful eye-tracking technique which consists of visual scenes that a participant views while listening to some spoken utterances. Participants look at the visual display which contains pictures of objects, meanwhile, the eye tracker monitors the eye movements as the heard language unfolds over time. The spoken utterance can extend from a single word to short narrative or even a story, and the utterance can be related to more than one object. When the eye movement of a saccade (a quick change of gaze moves from one point of fixation to another) happens, the eye tracker yields a time estimation at which word and corresponding picture has been identified on the visual scene while listening to the utterance. In this respect, VWP examines the relationship between the visual stimuli and language input, which also offers a different insight of exploring the integration of visual and language processing happening in the brain @huettig_using_2011, @schmid_eye-tracking_2016

How people interpret and process spoken language in the context of their contemporary visual field and how eye movements are involved as the language is being processed were first demonstrated experimentally by @cooper_control_1974. However, the vast majority of researchers have not considered the effects of Cooper’s study, and there is a relatively small body of literature that is concerned with the language processing and eye movements. 

After a few decades, since @tanenhaus_integration_1995 introduced the influential study of VWP in real-time spoken language comprehension, the rapid development of VWP started to flourish in the field, additionally with the contemporary high quality eye-trackers, VWP also has a transformative influence in psycholinguistics. More recent attention has focused on the provision of cross-language influences @elgort_cross-language_2023.


A number of studies have begun to examine the interplay between various themes of linguistics and visual information processing, which ranges from lexical to sentence and discourse level. @cooper_control_1974 investigated participants' behaviour while listening to short narratives, which involved semantic relation between objects. However, @tanenhaus_integration_1995 conducted the experiment differently to focus on unambiguous counterparts that concludes one-referent and two-referent context in each stimuli pairs, which mediated the syntactic context processing in VWP. Later trends in VWP have led to a growth of studies based on different themes of language comprehension. 

For instance, Altmann @altmann_thematic_1999 parsed the thematic role that is assigned in a context,  such as, whether the sentence “He drank some…” is plausible given the context which either did or did not introduce something drinkable. Moreover, @altmann_incremental_1999 addressed the incremental interpretation of verbs, which demonstrated that anticipatory eye movements could be predicted based on the verb used in a sentence. For example, upon hearing a verb like "eat," participants' gaze would quickly shift to an edible object in the scene before the object itself was mentioned, indicating that verb meaning constrains visual attention.

This ability to predict upcoming referents based on verb semantics highlights the close interaction between language processing and visual attention, providing crucial insights into how language guides our interpretation of the visual world. The original @altmann_incremental_1999 study used static images, focusing on how specific verbs direct gaze towards the most semantically appropriate object among a set of alternatives.

## Motivation

We want to build on top of the foundational work of Altmann & Kamide, by aiming to explore how dynamic visual stimuli might affect anticipatory eye movements within the VWP. Given that motion is a powerful cue in guiding visual attention, this study introduces video stimuli to examine whether the presence of motion has an effect on the predictive gaze patterns observed with static images.

The introduction of motion provides a new dimension to understanding the interaction between linguistic information and visual attention. While previous studies have shown that static images paired with predictive verbs can direct gaze, it remains unclear how motion in the visual scene impacts this process. 

## Research Question / Hypothesis
Building upon the results of @altmann_incremental_1999, which suggested that sentence processing is driven by predictive relationships between verbs and objects, our study aims to extend this understanding by introducing moving stimuli into the experimental setup. Specifically, we propose our research question:


<p style="text-align: center;">*Do people still make anticipatory eye movements <br> when presented with images that are in motion instead of static ones?*</p>
This question is grounded in the idea that while humans are naturally inclined to focus on moving objects @abrams_motion_2003, the extent to which this influences the predictive power of verbs during sentence processing remains unknown. We hypothesize that the addition of motion might modulate the anticipatory eye movements either by introducing competing visual stimuli that might disrupt the prediction.
