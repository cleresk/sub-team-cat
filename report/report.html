<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Elif Dilara Aygün">
<meta name="author" content="Karl Jorge Cleres Andreo">
<meta name="author" content="Yanhong Xu">
<meta name="dcterms.date" content="2024-08-29">

<title>Investigating the Visual World Paradigm</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="report_files/libs/clipboard/clipboard.min.js"></script>
<script src="report_files/libs/quarto-html/quarto.js"></script>
<script src="report_files/libs/quarto-html/popper.min.js"></script>
<script src="report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="report_files/libs/quarto-html/anchor.min.js"></script>
<link href="report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Investigating the Visual World Paradigm</h1>
            <p class="subtitle lead">Do people still make anticipatory eye movements when presented images with motion instead of static ones?</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-contents">
               <p>Elif Dilara Aygün </p>
               <p>Karl Jorge Cleres Andreo </p>
               <p>Yanhong Xu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 29, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-introduction" id="toc-sec-introduction" class="nav-link active" data-scroll-target="#sec-introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">1.1</span> Background</a></li>
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation"><span class="header-section-number">1.2</span> Motivation</a></li>
  <li><a href="#research-question" id="toc-research-question" class="nav-link" data-scroll-target="#research-question"><span class="header-section-number">1.3</span> Research Question</a></li>
  </ul></li>
  <li><a href="#sec-experiment" id="toc-sec-experiment" class="nav-link" data-scroll-target="#sec-experiment"><span class="header-section-number">2</span> Experiment</a>
  <ul class="collapse">
  <li><a href="#study-design" id="toc-study-design" class="nav-link" data-scroll-target="#study-design"><span class="header-section-number">2.1</span> Study Design</a></li>
  <li><a href="#sec-mat-stim" id="toc-sec-mat-stim" class="nav-link" data-scroll-target="#sec-mat-stim"><span class="header-section-number">2.2</span> Materials &amp; Stimuli</a></li>
  <li><a href="#sec-stdy-proc" id="toc-sec-stdy-proc" class="nav-link" data-scroll-target="#sec-stdy-proc"><span class="header-section-number">2.3</span> Study Procedure &amp; Apparatus</a></li>
  <li><a href="#sec-impl" id="toc-sec-impl" class="nav-link" data-scroll-target="#sec-impl"><span class="header-section-number">2.4</span> Implementation</a></li>
  </ul></li>
  <li><a href="#sec-analysis" id="toc-sec-analysis" class="nav-link" data-scroll-target="#sec-analysis"><span class="header-section-number">3</span> Analysis Methods</a>
  <ul class="collapse">
  <li><a href="#sec-logging" id="toc-sec-logging" class="nav-link" data-scroll-target="#sec-logging"><span class="header-section-number">3.1</span> Gathering Data &amp; Logging</a></li>
  <li><a href="#sec-pre-quality" id="toc-sec-pre-quality" class="nav-link" data-scroll-target="#sec-pre-quality"><span class="header-section-number">3.2</span> Preprocessing &amp; Quality Control</a></li>
  <li><a href="#measurements-metrics-visualisations" id="toc-measurements-metrics-visualisations" class="nav-link" data-scroll-target="#measurements-metrics-visualisations"><span class="header-section-number">3.3</span> Measurements, Metrics &amp; Visualisations</a></li>
  </ul></li>
  <li><a href="#sec-results" id="toc-sec-results" class="nav-link" data-scroll-target="#sec-results"><span class="header-section-number">4</span> Results</a>
  <ul class="collapse">
  <li><a href="#participants" id="toc-participants" class="nav-link" data-scroll-target="#participants"><span class="header-section-number">4.1</span> Participants</a></li>
  <li><a href="#pilot-data" id="toc-pilot-data" class="nav-link" data-scroll-target="#pilot-data"><span class="header-section-number">4.2</span> Pilot Data</a></li>
  <li><a href="#gaze-on-target-behavior" id="toc-gaze-on-target-behavior" class="nav-link" data-scroll-target="#gaze-on-target-behavior"><span class="header-section-number">4.3</span> Gaze on Target Behavior</a></li>
  </ul></li>
  <li><a href="#sec-discussion" id="toc-sec-discussion" class="nav-link" data-scroll-target="#sec-discussion"><span class="header-section-number">5</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#conclusion-outlook" id="toc-conclusion-outlook" class="nav-link" data-scroll-target="#conclusion-outlook"><span class="header-section-number">5.1</span> Conclusion &amp; Outlook</a></li>
  </ul></li>
  <li><a href="#contribution-table" id="toc-contribution-table" class="nav-link" data-scroll-target="#contribution-table"><span class="header-section-number">6</span> Contribution Table</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="..\report.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="sec-introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The Visual World Paradigm (VWP) <span class="citation" data-cites="allopenna_tracking_1998 huettig_using_2011">(<a href="#ref-allopenna_tracking_1998" role="doc-biblioref">1998</a>; <a href="#ref-huettig_using_2011" role="doc-biblioref">2011</a>)</span> is an established method in psycholinguistics that involves tracking eye movements to study how individuals process spoken language while viewing visual scenes. It allows researchers to understand how linguistic information influences attention and gaze behavior in real-time. As part of the course “Acquisition and Analysis of Eye-Tracking Data” we implemented the VWP in an experiment with a subsequent data analysis. In this report, we provide details to allow reproducibility of the experiment, discuss our study design choices and present our results.</p>
<section id="background" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="background"><span class="header-section-number">1.1</span> Background</h2>
<p>The role of language and visual processing has been studied by many researchers using eye-tracking and the VWP. VWP is a useful eye-tracking technique which consists of visual scenes that a participant views while listening to some spoken words. Participants look at the visual display which contains pictures of objects. Meanwhile, the eye-tracker monitors the eye movements as the heard language unfolds over time. The spoken utterance can extend from a single word to a short narrative or even a story, and it can be related to more than one object. When the eye movement of a saccade (a quick change of gaze from one point of fixation to another) happens, the eye-tracker yields a time estimation at which word and corresponding picture have been identified on the visual scene while listening to the utterance. In this respect, VWP examines the relationship between the visual stimuli and language input, which also offers a different insight into exploring the integration of visual and language processing happening in the brain <span class="citation" data-cites="huettig_using_2011 schmid_eye-tracking_2016">(<a href="#ref-huettig_using_2011" role="doc-biblioref">2011</a>; <a href="#ref-schmid_eye-tracking_2016" role="doc-biblioref">2016</a>)</span>.</p>
<p>How people interpret and process spoken language in the context of their contemporary visual field and how eye movements are involved as the language is being processed were first demonstrated experimentally by Cooper <span class="citation" data-cites="cooper_control_1974">(<a href="#ref-cooper_control_1974" role="doc-biblioref">1974</a>)</span>. However, the vast majority of researchers have not considered the effects of Cooper’s study, and there is a relatively small body of literature that is concerned with language processing and eye movements during that time.</p>
<p>After a few decades, since Tannenhaus et al. <span class="citation" data-cites="tanenhaus_integration_1995">(<a href="#ref-tanenhaus_integration_1995" role="doc-biblioref">1995</a>)</span> introduced the influential study of VWP in real-time spoken language comprehension, the rapid development of VWP started to flourish in the field. Additionally with the contemporary high-quality eye-trackers, VWP also has a transformative influence in psycholinguistics. More recent attention has focused on the provision of cross-language influences <span class="citation" data-cites="elgort_cross-language_2023">(<a href="#ref-elgort_cross-language_2023" role="doc-biblioref">2023</a>)</span>.</p>
<p>A number of studies have begun to examine the interplay between various themes of linguistics and visual information processing, which ranges from lexical to sentence and discourse level. Cooper <span class="citation" data-cites="cooper_control_1974">(<a href="#ref-cooper_control_1974" role="doc-biblioref">1974</a>)</span> first investigated participants’ behaviour while listening to short narratives, which involved semantic relations between objects. However, Tannenhaus et al. <span class="citation" data-cites="tanenhaus_integration_1995">(<a href="#ref-tanenhaus_integration_1995" role="doc-biblioref">1995</a>)</span> conducted the experiment differently to focus on unambiguous counterparts that conclude one-referent and two-referent contexts in each stimuli pair, which mediated the syntactic context processing in VWP. Later trends in VWP have led to a growth of studies based on different themes of language comprehension.</p>
<p>For instance, Altmann <span class="citation" data-cites="altmann_thematic_1999">(<a href="#ref-altmann_thematic_1999" role="doc-biblioref">1999</a>)</span> parsed the thematic role that is assigned in a context, such as whether the sentence “He drank some…” is plausible given the context which either did or did not introduce something drinkable. Moreover, Altmann &amp; Kamide <span class="citation" data-cites="altmann_incremental_1999">(<a href="#ref-altmann_incremental_1999" role="doc-biblioref">1999</a>)</span> addressed the incremental interpretation of verbs, which demonstrated that anticipatory eye movements could be predicted based on the verb used in a sentence. For example, upon hearing a verb like “eat,” participants’ gaze would quickly shift to an edible object in the scene before the object itself was mentioned, indicating that verb meaning constrains visual attention.</p>
<p>This ability to predict upcoming referents based on verb semantics highlights the close interaction between language processing and visual attention, providing crucial insights into how language guides our interpretation of the visual world. The original study used static images, focusing on how specific verbs direct gaze towards the most semantically appropriate object among a set of alternatives.</p>
</section>
<section id="motivation" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="motivation"><span class="header-section-number">1.2</span> Motivation</h2>
<p>We want to build on top of the foundational work of Altmann &amp; Kamide, by aiming to explore how dynamic visual stimuli might affect anticipatory eye movements within the VWP. Given that motion is a powerful cue in guiding visual attention, this study introduces video stimuli to examine whether the presence of motion has an effect on the predictive gaze patterns observed with static images.</p>
<p>The introduction of motion provides a new dimension to understanding the interaction between linguistic information and visual attention. While previous studies have shown that static images paired with predictive verbs can direct gaze, it remains unclear how motion in the visual scene impacts this process.</p>
</section>
<section id="research-question" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="research-question"><span class="header-section-number">1.3</span> Research Question</h2>
<p>Building upon the results of Altmann &amp; Kamide <span class="citation" data-cites="altmann_incremental_1999">(<a href="#ref-altmann_incremental_1999" role="doc-biblioref">1999</a>)</span>, which suggested that sentence processing is driven by predictive relationships between verbs and objects, our study aims to extend this understanding by introducing moving stimuli into the experimental setup. Specifically, we propose our research question:</p>
<p style="text-align: center;">
<em>Do people still make anticipatory eye movements <br> when presented with images that are in motion instead of static ones?</em>
</p>
<p>This question is grounded in the idea that while humans are naturally inclined to focus on moving objects <span class="citation" data-cites="abrams_motion_2003">(<a href="#ref-abrams_motion_2003" role="doc-biblioref">2003</a>)</span>, the extent to which this influences the predictive power of verbs during sentence processing remains unknown. We expect that the addition of motion might modulate the anticipatory eye movements by introducing competing visual stimuli that might disrupt the prediction.</p>
</section>
</section>
<section id="sec-experiment" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Experiment</h1>
<p>In this chapter, we go over the methodology details of our experiment. We are interested in comparing the gaze behavior of the participants when they are presented with static or moving images accompanied by a related audible sentence. This was done by collecting gaze data and analyzing the gaze behavior of the participants. Further details on the analysis can be found in (<a href="#sec-analysis" class="quarto-xref">Section&nbsp;3</a>).</p>
<section id="study-design" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="study-design"><span class="header-section-number">2.1</span> Study Design</h2>
<p>We have a between-subject study design with one factor being the visual stimuli type. The factor has two possible levels, with one being static and the other in motion. The participants are exposed to all 64 audio stimuli, but whether the corresponding visual stimuli are static or in motion depends on their participant group, which was determined by their randomly assigned participant ID. For instance, if for stimuli nr. 5 the visual was static for participant group “Chicken”, the stimuli type was simply flipped for the other group “Wolf”. With this latin square design, we made sure that the stimuli/condition mappings were equally distributed among both participant groups, as long as the number of participants in the groups was the same. The order of the stimuli overall was random for each participant by utilizing the random order function within OpenSesame. In summary, we have 64 audio stimuli with two possible matching visual stimuli. The participants were shown 32 visual stimuli for each category, which was determined by their randomly assigned participant group. Each trial started with a fixation dot for one second, followed by a preview of the four images for another second. Then, the audio started playing and the images were still shown 500 milliseconds after audio completion, after which the experiment switched back to the fixation dot and the trial was completed. The participant is then able to proceed to the next trial by looking at the fixation dot. <br></p>
<p>A visualization of the trial sequence is shown in <a href="#fig-trials" class="quarto-xref">Figure&nbsp;1</a>:</p>
<div id="fig-trials" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trials-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig2_2_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trials-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Experiment procedure with the trial loop.
</figcaption>
</figure>
</div>
</section>
<section id="sec-mat-stim" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-mat-stim"><span class="header-section-number">2.2</span> Materials &amp; Stimuli</h2>
<p>We have 64 audio stimuli in total with two matching visual stimulus versions, one consisting of four regular images and the other with four GIFs. The audio stimuli consisted of different sentences with the same structure: [location/time], [person] [verb] [object], e.g.: “At breakfast, the girl ate a croissant.” The corresponding visual stimuli consists of four images with only one being closely related to the sentence. In the case of our example, we have one image being a croissant and the remaining images being random inedible objects that are not related to breakfast. For the motion stimuli, we chose matching GIFs for the four static images. We tried to make sure that the GIFs had a smimilar speed. The exact specifications for static and motion stimuli as well as the audio stimuli can be found in the README-File of the experiment.<br>
We came up with the 64 different sentences by following the described structure and created corresponding audios with a free text-to-speech tool<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. For the visual stimuli, we re-used most of the images from the visual world paradigm tutorial by OpenSesame <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and looked for additional ones using Google images.</p>
<p>The stimuli creation process turned out to be a cumbersome process with handling the text-to-speech tool to include breaks and not speak too fast, finding visual stimuli to go with the audios and making sure that there is only one matching target sub-stimulus for each sentence. With our amount of stimuli, it is easy to lose sight of all the different sub-images. Therefore, to make the stimuli creation process easier, we used a central Excel sheet as a documentation <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> where all the sentences and their corresponding sub-stimuli file names were stated, including which stimulus version (static or motion) would be shown for each condition group. In case something changed, we only had to change the central document and paste its contents to our trial loop in OpenSesame. In <a href="#fig-stimuli" class="quarto-xref">Figure&nbsp;2</a> we provide some stimuli examples:</p>
<div id="fig-stimuli" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stimuli-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig2_3_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stimuli-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Two example sentences that we used in the experiment and their respective static and motion visual stimuli.
</figcaption>
</figure>
</div>
</section>
<section id="sec-stdy-proc" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-stdy-proc"><span class="header-section-number">2.3</span> Study Procedure &amp; Apparatus</h2>
<p>To investigate our research question, we needed to invite participants to partake in our experiment. <br> We recruited participants by asking for volunteers in our circle of acquaintances and invited the interested people between mid-July and August. Our target sample size was at least ten participants to end up with at least five people per group. Once a participant arrived, they were given an information sheet with a brief description of the experiment, what kind of devices were going to be used and their corresponding safety risks. Additionally, they were asked to sign a standard consent form for studies with humans that are conducted at the university. To counterbalance the number of participants per participant group, we made them pick a card with a unique number between 1-10 and depending on whether the ID was an odd number or not, they got assigned to the corresponding group. The participants were asked to make themselves comfortable on the chair and adjust it according to their preference, the same goes for the standard chin rest we used. Both the chair and the chin rest were disinfected before each participant arrived. To execute the experiment, we used a standard desktop PC with two displays standing back-to-back in duplicate view, where one was directed to the participant and the other was used by the study conductor to start and supervise the experiment. For tracking, we used the stationary GazePoint GP3 HD eye-tracker in 150 Hz mode. When starting the experiment in OpenSesame, it starts out with initializing the eye-tracker and calibrating. The calibration process sometimes took multiple attempts, until the results looked precise and accurate enough. We always told the participants that the calibration process can require multiple attempts to make sure that they do not consider themselves as doing anything wrong. After the setup, the experiment itself lasted around 12 to 15 minutes. The participants had the control to proceed to the next trial at their own speed and they were informed, that they could take breaks in-between trials if needed. Finally, we asked the participants for some basic demographic information and their previous experiences with eye-tracking. We placed the demographic survey at the end of the experiment to avoid stereotype threat (see <span class="citation" data-cites="spencer2016stereotype">(<a href="#ref-spencer2016stereotype" role="doc-biblioref">2016</a>)</span>).</p>
</section>
<section id="sec-impl" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-impl"><span class="header-section-number">2.4</span> Implementation</h2>
<p>The experiment was conducted using the most recent version of OpenSesame 4.0 Melodramatic Milgram with the PyGame backend. The logic is divided into three distinct logical units. <br> The first unit is used to load custom configuration and preprocess static and motion stimuli. The configuration file was managed with the YAML parser and emitter PyYAML <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and used to customize texts, their layouts and positions, fixation dots, positions of stimuli as well as the general timing of the experiment. This way we kept the experiment flexible for minor changes during the pilot studies or possible future research. Since it was not possible to display videos or GIFs in OpenSesame, we used the Python library OpenCV <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> to extract the frames of each mp4 video file and save them into a separate folder for each stimulus inside a temporary directory. This directory can be specified in the custom configuration file stated above. <br> In the second unit of the experiment, the built-in calibration of OpenSesame was used to calibrate the eye-tracker and the group selection was displayed to the user. <br> The third unit consists of the trial loop. For each trial, the user is first shown a fixation dot, then the four substimuli and afterwards the sentence to the pictures is played as stated in <a href="#sec-stdy-proc" class="quarto-xref">Section&nbsp;2.3</a>. The trial was designed in such a way that it would only commence once the user had fixated on the central fixation dot. In order to maintain a consistent framerate for each motion sub-stimulus, all the images were placed into an infinite loop, with each sub-stimulus containing an individual frame counter. This counter was reset to the beginning, once it reached the final frame. Any concerns regarding the performance of the experiment could be cleared after extensive piloting. Once the designated sentence was played, the infinite loop was interrupted and, after a small delay, the pause screen was displayed. The logging took place multiple times during the trial loop and finally logged within the OpenSesame logger. Logging will be discussed further in <a href="#sec-logging" class="quarto-xref">Section&nbsp;3.1</a>.</p>
</section>
</section>
<section id="sec-analysis" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Analysis Methods</h1>
<p>This chapter shows how the data was gathered, preprocessed and analyzed in order to answer the research question. The analysis was performed in Python 3.12.4 <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> with the use of the libraries Pandas <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> and NumPy <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> for data manipulation and analysis as well as Matplotlib <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> and Seaborn <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> for visualizations. The resulting scripts are Jupyter Notebooks that, depending on the filename, can be run for a single subject <em>(e.g.&nbsp;analysis-subjects.ipynb)</em> or for all the subjects <em>(e.g.&nbsp;analysis-global.ipynb)</em>.</p>
<section id="sec-logging" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-logging"><span class="header-section-number">3.1</span> Gathering Data &amp; Logging</h2>
<p>In order to calibrate, record and log the gaze data in OpenSesame, we used several built-in pygaze elements. For instance, the <em>pygaze_init</em> element is placed before the trial loop in order to perform the calibration. Other elements include the <em>pygaze_start_recording</em> placed at the beginning of a trial followed by a logger and a <em>pygaze_stop_recording</em> at the end of a trial. <br></p>
<p>It is essential to record the occurrence of specific events during the experiment, as these will be subject to our subsequent analysis. This is achieved by custom logging with the logger item in OpenSesame. The structure of a custom log is always consistent. It comprises a string containing all the information from the current row in the stimuli table. The string is formatted in a way that each cell of the row is logged as a key-value pair and divided by a semicolon.</p>
<blockquote class="blockquote">
<p>VAR TRIAL_LOG VERB_CUE=EAT;GROUP=CHICKEN;SENTENCE_ID=11; SENTENCE=AT THE FARM, THE GIRL FED THE CHICKEN. ; (…)</p>
</blockquote>
<p>In order to facilitate the differentiation of the current event, a key-value pair is appended to the log string. Four potential events are logged within the experiment, as follows:</p>
<ul>
<li>Fixation: This event is logged at the beginning of a trial when the participant is presented with the fixation dot on the screen.</li>
<li>Preview: Stimulus of a trial is presented.</li>
<li>Audiostart: The audio with the sentence starts playing.</li>
<li>Pause: The beginning of the pause, where the user has to look at the fixation dot in order to continue.</li>
</ul>
<p>The subsequent analysis contains two more events which could not be logged in OpenSesame due to its design. Our solution to this problem will be presented in the next chapter.</p>
</section>
<section id="sec-pre-quality" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-pre-quality"><span class="header-section-number">3.2</span> Preprocessing &amp; Quality Control</h2>
<p>In the beginning, we filtered out the data that was not relevant to this research. The dropped columns can be seen in <a href="#lst-del" class="quarto-xref">Listing&nbsp;1</a>:</p>
<div id="lst-del" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="1">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-del-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1
</figcaption>
<div aria-describedby="lst-del-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (...) read data as pandas.DataFrame</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># drop irrelevant data</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.drop([<span class="st">"TIME_TICK"</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.drop([<span class="st">"FPOGX"</span>, <span class="st">"FPOGY"</span>, <span class="st">"FPOGS"</span>, <span class="st">"FPOGD"</span>, <span class="st">"FPOGID"</span>, <span class="st">"FPOGV"</span>], axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.drop([<span class="st">"LPOGX"</span>, <span class="st">"LPOGY"</span>, <span class="st">"LPOGV"</span>, <span class="st">"RPOGX"</span>, <span class="st">"RPOGY"</span>, <span class="st">"RPOGV"</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.drop([<span class="st">"LPCX"</span>, <span class="st">"LPCY"</span>, <span class="st">"LPD"</span>, <span class="st">"LPS"</span>, <span class="st">"LPV"</span>, <span class="st">"RPCX"</span>, <span class="st">"RPCY"</span>, <span class="st">"RPD"</span>, <span class="st">"RPS"</span>, <span class="st">"RPV"</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.drop([<span class="st">"LEYEX"</span>, <span class="st">"LEYEY"</span>, <span class="st">"LEYEZ"</span>, <span class="st">"LPUPILD"</span>, <span class="st">"LPUPILV"</span>, <span class="st">"REYEX"</span>, <span class="st">"REYEY"</span>, <span class="st">"REYEZ"</span>, <span class="st">"RPUPILD"</span>, <span class="st">"RPUPILV"</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.drop([<span class="st">"CX"</span>, <span class="st">"CY"</span>, <span class="st">"CS"</span>], axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<p>The resulting Dataframe looks like this:</p>
<div id="tbl-a" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Filtered raw gaze data of a participant
</figcaption>
<div aria-describedby="tbl-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">TIME</th>
<th style="text-align: center;">BPOGX</th>
<th style="text-align: center;">BPOGY</th>
<th style="text-align: center;">BPOGV</th>
<th style="text-align: center;">USER</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">204.69383</td>
<td style="text-align: center;">0.33433</td>
<td style="text-align: center;">0.57871</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">START_TRIAL 0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The resulting data and its interpretation can be found in the GazePoint API documentation <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> and forms the basis for the analysis. <br></p>
<p>At the beginning, several sanity checks were performed in order to evaluate the quality of the collected data. The first one is the examination of the recorded logs. Therefore, the occurrences of all existing logs in the USER column were counted. Since the experiment has 64 trials there should be 64 hits for each log message except the <em>TRIAL_WARNING</em> which is only logged when the subject does not fixate on the fixation dot during a pause for more than 15 seconds. Afterwards, the results are plotted in a barplot (see <a href="#fig-barplt-evts" class="quarto-xref">Figure&nbsp;3</a> and <a href="#lst-barplot-events" class="quarto-xref">Listing&nbsp;2</a>). <br></p>
<div id="fig-barplt-evts" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barplt-evts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig3_2_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barplt-evts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Counted vs.&nbsp;expected logs for an example subject
</figcaption>
</figure>
</div>
<div id="lst-barplot-events" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="2">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-barplot-events-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2
</figcaption>
<div aria-describedby="lst-barplot-events-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># finds the occurencies of "word" in data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_occur(data, word):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    all_text <span class="op">=</span> <span class="st">" "</span>.join(data[<span class="st">"USER"</span>].astype(<span class="bu">str</span>).values.flatten())</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    total_count <span class="op">=</span> all_text.count(word)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_count</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># (...)</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># index</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>log_index <span class="op">=</span> [<span class="st">"START_TRIAL"</span>, <span class="st">"EVENT=FIXATION"</span>, <span class="st">"EVENT=PREVIEW"</span>, <span class="st">"EVENT=AUDIOSTART"</span>, <span class="st">"EVENT=PAUSE"</span>, <span class="st">"STOP_TRIAL"</span>, <span class="st">"TRIAL_WARNING"</span>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>ind <span class="op">=</span> np.arange(<span class="bu">len</span>(log_index))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># count occurencies of each word</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> []</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> log <span class="kw">in</span> log_index:</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    count.append(find_occur(data, log))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># load data into dataframe</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>logs <span class="op">=</span> pd.DataFrame({<span class="st">"count"</span>: count, <span class="st">"expected"</span>: expected_logs}, index<span class="op">=</span>log_index)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># (...) plot data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<p>By plotting a histogram of the time delta between samples, we found out that it is not consistent. Therefore, we interpolated the values of BPOGX, BPOGY, and BPOGV with the scipy <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> linear (one-dimensional) interpolator (see <a href="#lst-interp-code" class="quarto-xref">Listing&nbsp;3</a>). The time was reorganized in a way that it matches the sampling rate. Since the eye-tracker has a sampling rate of 150 Hz, the total time of the experiment in seconds was multiplied by 150 to figure out the number of bins for the interpolation function. The respective values for BPOGX, BPOGY and BPOGV at the location of each bin determined the interpolated value. In order to obtain a valid BPOGV, its value after interpolation was rounded to an integer value. <br></p>
<div id="lst-interp-code" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="3">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-interp-code-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3
</figcaption>
<div aria-describedby="lst-interp-code-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (...)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># interpolate BPOGX and BPOGY</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>interp_func_x <span class="op">=</span> interp1d(data[<span class="st">"TIME"</span>], data[<span class="st">"BPOGX"</span>], kind<span class="op">=</span><span class="st">"linear"</span>, fill_value<span class="op">=</span><span class="st">"interpolate"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>interp_func_y <span class="op">=</span> interp1d(data[<span class="st">"TIME"</span>], data[<span class="st">"BPOGY"</span>], kind<span class="op">=</span><span class="st">"linear"</span>, fill_value<span class="op">=</span><span class="st">"interpolate"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>interp_func_v <span class="op">=</span> interp1d(data[<span class="st">"TIME"</span>], data[<span class="st">"BPOGV"</span>], kind<span class="op">=</span><span class="st">"linear"</span>, fill_value<span class="op">=</span><span class="st">"interpolate"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># generate new evenly spaced time points</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>t_delta <span class="op">=</span> data[<span class="st">"TIME"</span>].<span class="bu">max</span>() <span class="op">-</span> data[<span class="st">"TIME"</span>].<span class="bu">min</span>() </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>lin_num <span class="op">=</span> (t_delta <span class="op">*</span> sample_rate).<span class="bu">round</span>(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>new_time <span class="op">=</span> np.linspace(data[<span class="st">"TIME"</span>].<span class="bu">min</span>(), data[<span class="st">"TIME"</span>].<span class="bu">max</span>(), num<span class="op">=</span>lin_num)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># get interpolated values with new time</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>new_bpogx <span class="op">=</span> interp_func_x(new_time)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>new_bpogy <span class="op">=</span> interp_func_y(new_time)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>new_bpogv <span class="op">=</span> interp_func_v(new_time)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># overwrite original dataframe and round like original data</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({})</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>data[<span class="st">"TIME"</span>] <span class="op">=</span> new_time</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>data[<span class="st">"BPOGX"</span>] <span class="op">=</span> new_bpogx.astype(<span class="bu">float</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>data[<span class="st">"BPOGY"</span>] <span class="op">=</span> new_bpogy.astype(<span class="bu">float</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>data[<span class="st">"BPOGV"</span>] <span class="op">=</span> new_bpogv.<span class="bu">round</span>(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_nearest_index(array, value):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> (np.<span class="bu">abs</span>(array <span class="op">-</span> value)).argmin()</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> idx</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co"># map user events back to correct time</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index, row <span class="kw">in</span> before_interpol_data.iterrows():</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pd.notna(row[<span class="st">"USER"</span>]):  <span class="co"># Check if USER event is present</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        nearest_index <span class="op">=</span> find_nearest_index(new_time, row[<span class="st">"TIME"</span>])</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add or append the event to the USER column</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        data.at[nearest_index, <span class="st">"USER"</span>] <span class="op">=</span> row[<span class="st">"USER"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<p><a href="#fig-interpolation" class="quarto-xref">Figure&nbsp;4</a> shows an example function before and after interpolation:</p>
<div id="fig-interpolation" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interpolation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig3_2_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interpolation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Interpolation
</figcaption>
</figure>
</div>
<p>In the next step, the custom logs were decoded. In this process, the rows of the dataframe containing a <em>TRIAL_LOG</em> in the <em>USER</em> column were filtered and the logsting was decoded. Therefore all key-value pairs were split and for each key, a new column was created. The respective value was placed in the current row of the key-column. Afterwards, the <em>USER</em> column was dropped. A row that contained a log looks could look like this:</p>
<div id="tbl-b" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Decoded custom logs in dataframe with gaze data
</figcaption>
<div aria-describedby="tbl-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">TIME</th>
<th style="text-align: center;">(…)</th>
<th style="text-align: center;">BPOGV</th>
<th style="text-align: center;">SUBJECT</th>
<th style="text-align: center;">EVENT</th>
<th style="text-align: center;">TRIAL</th>
<th style="text-align: center;">GROUP</th>
<th style="text-align: center;">(…)</th>
<th style="text-align: center;">VERB_CUE</th>
<th style="text-align: center;">TARGET_CUE_TIMING</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1487.3…</td>
<td style="text-align: center;">(…)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">FIXATION</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">WOLF</td>
<td style="text-align: center;">(…)</td>
<td style="text-align: center;">SERVED</td>
<td style="text-align: center;">2115</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="lst-cust-log-dec" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="4">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-cust-log-dec-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4
</figcaption>
<div aria-describedby="lst-cust-log-dec-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index, row <span class="kw">in</span> data.iterrows():</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add subject to every row</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    data.at[index, <span class="st">"SUBJECT"</span>] <span class="op">=</span> subject_no</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get loggin row of opengaze data</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    inp <span class="op">=</span> <span class="bu">str</span>(row[<span class="st">"USER"</span>])</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> inp.find(<span class="st">"TRIAL_LOG"</span>) <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        cleaned_inp <span class="op">=</span> inp.split(<span class="st">"VAR TRIAL_LOG "</span>, maxsplit<span class="op">=</span><span class="dv">2</span>)[<span class="dv">1</span>]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        kv_inp <span class="op">=</span> cleaned_inp.split(<span class="st">";"</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># decode key/value-pairs and copy them to column (key) and value (cell)</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> elem <span class="kw">in</span> kv_inp:</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            key, val <span class="op">=</span> elem.split(<span class="st">"="</span>, maxsplit<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            data.at[index, key] <span class="op">=</span> val</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># other custom logs</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> inp.find(<span class="st">"TRIAL_WARNING"</span>) <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        data.at[index, <span class="st">"EVENT"</span>] <span class="op">=</span> <span class="st">"WARNING"</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> inp.find(<span class="st">"START_TRIAL"</span>) <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        data.at[index, <span class="st">"EVENT"</span>] <span class="op">=</span> <span class="st">"START_TRIAL"</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        data.at[index, <span class="st">"TRIAL"</span>] <span class="op">=</span> <span class="bu">int</span>(inp[<span class="dv">11</span>:].replace(<span class="st">" "</span>, <span class="st">""</span>))</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> inp.find(<span class="st">"STOP_TRIAL"</span>) <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        data.at[index, <span class="st">"EVENT"</span>] <span class="op">=</span> <span class="st">"STOP_TRIAL"</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        data.at[index, <span class="st">"TRIAL"</span>] <span class="op">=</span> <span class="bu">int</span>(inp[<span class="dv">11</span>:].replace(<span class="st">" "</span>, <span class="st">""</span>))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="co"># drop original log column</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>data.drop(columns<span class="op">=</span>[<span class="st">"USER"</span>], inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<p>As mentioned in <a href="#sec-logging" class="quarto-xref">Section&nbsp;3.1</a>, two events can not be logged due to the design of OpenSesame. These events are <em>VERBONSET</em> and <em>TARGETONSET</em>. <em>VERBONSET</em> refers to the time, when the sentence is played to the subject and the verb of the sentence is said. <em>TARGETONSET</em> is the exact timing where the target word of the sentence is spoken. <a href="#sec-mat-stim" class="quarto-xref">Section&nbsp;2.2</a> introduced the stimuli table <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> which also contains for each sentence the audio and the time that has passed until the verb and the target word are said. We consider these two time intervals as deltas. Afterwards, we took the time where the <em>AUDIOSTART</em> event happened and added each time interval to the <em>AUDIOSTART</em>-time. The resulting times are the values <em>VERB- and TARGETONSET</em>. <a href="#fig-verbtargetonset" class="quarto-xref">Figure&nbsp;5</a> illustrates this process.</p>
<div id="fig-verbtargetonset" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-verbtargetonset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig3_2_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-verbtargetonset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Connection between event cues AUDIOSTART, VERB- and TARGETONSET
</figcaption>
</figure>
</div>
<div id="lst-cust-calc-verb-taget-onset" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="5">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-cust-calc-verb-taget-onset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5
</figcaption>
<div aria-describedby="lst-cust-calc-verb-taget-onset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extend logs with the corresponding events</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>audio_start_data <span class="op">=</span> data.query(<span class="st">"EVENT=='AUDIOSTART'"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index, row <span class="kw">in</span> audio_start_data.iterrows():</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    sample_time <span class="op">=</span> <span class="bu">float</span>(row[<span class="st">"TIME"</span>])</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    delta_verb_onset <span class="op">=</span> <span class="bu">float</span>(row[<span class="st">"VERB_CUE_TIMING"</span>])<span class="op">/</span><span class="dv">1000</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    delta_target_onset <span class="op">=</span> <span class="bu">float</span>(row[<span class="st">"TARGET_CUE_TIMING"</span>])<span class="op">/</span><span class="dv">1000</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    time_verb_onset <span class="op">=</span> sample_time <span class="op">+</span> delta_verb_onset</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    time_target_onset <span class="op">=</span> sample_time <span class="op">+</span> delta_target_onset</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    first_sample_verb <span class="op">=</span> data[data[<span class="st">"TIME"</span>] <span class="op">&gt;=</span> time_verb_onset].iloc[<span class="dv">0</span>].name</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    first_sample_target <span class="op">=</span> data[data[<span class="st">"TIME"</span>] <span class="op">&gt;=</span> time_target_onset].iloc[<span class="dv">0</span>].name</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> field <span class="kw">in</span> fields_to_copy:</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        data.at[first_sample_verb, field] <span class="op">=</span> row[field]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        data.at[first_sample_target, field] <span class="op">=</span> row[field]</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the EVENT to "VERBONSET"</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    data.at[first_sample_verb, <span class="st">"EVENT"</span>] <span class="op">=</span> <span class="st">"VERBONSET"</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    data.at[first_sample_target, <span class="st">"EVENT"</span>] <span class="op">=</span> <span class="st">"TARGETONSET"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<!--
In order to work better with pandas queries, the logs were expanded. That means, that we iterate over all custom logs that were added during this process so far and copy them to the next empty row. If the next row is not empty, the data of the next row gets copied to the next but one row. This process looks like the following:

```
1: 1 | 3 | FIXATION | 0 | WOLF | (...) | SERVED | 2115 |
2:                                                       <- custom logs of row 1 are copied here
3:                                                       <- custom logs of row 1 are copied here
5: 1 | 3 | PREVIEW  | 0 | WOLF | (...) | SERVED | 2115 |
6:                                                       <- custom logs of row 5 are copied here
7:                                                       <- (...)
```

::: {#fig-log-expansion .cell execution_count=6}
``` {.python .cell-code}
# make sure event gets copyied as well
if "EVENT" not in fields_to_copy:
    fields_to_copy.append("EVENT")

found_first_evt = False
for index, row in data.iterrows():

    # skip first samples with NaN
    if "START_TRIAL" in str(data.at[index, "EVENT"]):
        found_first_evt = True
        continue

    if not found_first_evt:
        continue
    
    # copy last log if custom log cells are NaN
    if pd.isna(data.at[index, "EVENT"]):
        
        for field in fields_to_copy:
            data.at[index, field] = data.at[index-1, field]
```
:::


-->
<p>Finally, the format of rows that contained mixed characters was unified and the resulting dataframe was exported into a csv file under the <em>~/data/preprocessed/</em> directory.</p>
<div id="lst-format-check" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="7">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-format-check-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6
</figcaption>
<div aria-describedby="lst-format-check-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>data[<span class="st">"SUBJECT"</span>] <span class="op">=</span> data[<span class="st">"SUBJECT"</span>].fillna(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>data[<span class="st">"TRIAL"</span>] <span class="op">=</span> data[<span class="st">"TRIAL"</span>].fillna(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>data[<span class="st">"SENTENCE_ID"</span>] <span class="op">=</span> data[<span class="st">"SENTENCE_ID"</span>].fillna(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<p>This concludes the <a href="#sec-pre-quality" class="quarto-xref">Section&nbsp;3.2</a> chapter. The full script for the preprocessing is enclosed in the form of a Jupyter notebook and can be found in the project path <em>~/scripts/preprocess-subject.ipynb</em>.</p>
</section>
<section id="measurements-metrics-visualisations" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="measurements-metrics-visualisations"><span class="header-section-number">3.3</span> Measurements, Metrics &amp; Visualisations</h2>
<p>To verify the correctness, consistency and accuracy of the data collected, especially during the piloting trials, several visualizations were employed. The first one is the visualization of the BPOGX and BPOGY coordinates generated (see Fig. <a href="#fig-bpogxy" class="quarto-xref">Figure&nbsp;6</a>). This plot illustrated the raw gaze points captured by the eye-tracker across the screen.</p>
<div id="fig-bpogxy" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bpogxy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig4_4_0.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bpogxy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Example of best point of gaze for a sentence
</figcaption>
</figure>
</div>
<p>The scanpath visualization (see <a href="#fig-scanpath" class="quarto-xref">Figure&nbsp;7</a>) was used to trace the path that a subject’s eye movements took to ensure alignment with the expected gaze patterns. The heatmaps (see <a href="#fig-heatmap" class="quarto-xref">Figure&nbsp;8</a>) allowed a quick verification of attention directed towards the defined areas of interest (AOI).</p>
<div id="fig-scanpath" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scanpath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig4_4_1.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scanpath-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Example scanpath for a sentence
</figcaption>
</figure>
</div>
<div id="fig-heatmap" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig4_4_2.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-heatmap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Example heatmap for a sentence
</figcaption>
</figure>
</div>
<p>To determine whether a participant directed their gaze toward a target, two primary types of eye movements can be looked at: fixations and saccades. <br></p>
<p>When considering saccades, several variations of this metric might be interesting to see whether the anticipation effect persists when adding motion stimuli. One example would be the first saccade after the target word of a sentence is said. However, the inclusion of such a metric requires precise experimental timing and raises several methodological questions. For example, how should saccades that initiate immediately after the target cue get handled? Overall, saccades are not sufficiently robust for this analysis and introduce a lot of noise, making them impractical within the scope of this project. <br></p>
<p>Conversely, fixations provide a more straightforward metric. In this study, we are not interested in the duration of fixations or the precise location of the gaze point. Instead, the primary focus is on determining whether the participant is looking at a specific target. Given this requirement, we chose not to analyze fixations directly. Instead, we opted to use the valid samples from the eye-tracker. These samples are more robust and less noisy since they do not need to undergo a fixation detection algorithm, which can create additional noise. <br></p>
<p>Given the need to accurately detect the specific sub-stimulus at which a participant is looking, four areas of interest were defined. These AOIs correspond to the locations of each sub-stimulus and are slightly larger (350 pixels) than the resolution of each sub-stimulus (300 pixels). This setup results in four AOIs: top-left (TL), top-right (TR), bottom-left (BL), and bottom-right (BR). The center of each AOI corresponds to the relative position of each sub-stimulus to the middle. For example, the center top-left AOI is 1/4 of the width and 1/4 of the height. An algorithm was implemented that considers the center positions and calculates two intervals where the BPOGX and BPOGY of a sample need to be located, in order to be considered inside the bounding box (see <a href="#lst-alg-aoi" class="quarto-xref">Listing&nbsp;7</a>). Since our metrics based on AOIs, the algorithm was tested with additional visualisations (see <a href="#fig-aoi-valid" class="quarto-xref">Figure&nbsp;9</a>) and could be used in the further course of the analysis.</p>
<div id="lst-alg-aoi" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="8">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-alg-aoi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7
</figcaption>
<div aria-describedby="lst-alg-aoi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># function that checks wheather subject looked at desired area of interest</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bpog_in_target_bbox(bpogx, bpogy, pos):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># var "width" and "height" are globally (static) definded</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> bpogx <span class="op">*</span> width</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> bpogy <span class="op">*</span> height</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pos <span class="op">==</span> <span class="st">"TL"</span>:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        relpos <span class="op">=</span> (width<span class="op">/</span><span class="dv">4</span>, height<span class="op">/</span><span class="dv">4</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> pos <span class="op">==</span> <span class="st">"TR"</span>:</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        relpos <span class="op">=</span> (width<span class="op">*</span>(<span class="dv">3</span><span class="op">/</span><span class="dv">4</span>), height<span class="op">/</span><span class="dv">4</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> pos <span class="op">==</span> <span class="st">"BL"</span>:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        relpos <span class="op">=</span> (width<span class="op">/</span><span class="dv">4</span>, height<span class="op">*</span>(<span class="dv">3</span><span class="op">/</span><span class="dv">4</span>))</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> pos <span class="op">==</span> <span class="st">"BR"</span>:</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        relpos <span class="op">=</span> (width<span class="op">*</span>(<span class="dv">3</span><span class="op">/</span><span class="dv">4</span>), height<span class="op">*</span>(<span class="dv">3</span><span class="op">/</span><span class="dv">4</span>))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        relpos <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add/rest 150 + 50 to relative position </span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    pos_x_l <span class="op">=</span> relpos[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">200</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    pos_x_r <span class="op">=</span> relpos[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">200</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    pos_y_d <span class="op">=</span> relpos[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">200</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    pos_y_u <span class="op">=</span> relpos[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">200</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check if in area of interest box</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (x <span class="op">&gt;</span> pos_x_l) <span class="kw">and</span> (x <span class="op">&lt;</span> pos_x_r):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(y <span class="op">&gt;</span> pos_y_d) <span class="kw">and</span> (y <span class="op">&lt;</span> pos_y_u):</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<div id="fig-aoi-valid" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-aoi-valid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig4_4_10.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-aoi-valid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Area of interest verification algorithm
</figcaption>
</figure>
</div>
<p>With the area of interest algorithm, we can now have a look at the two metrics of this experiment. The target ratio and the non-target ratio. The target ratio describes the samples that were detected inside the target area of interest, in relation to the total amount of examples. The non-target ratio describes the samples that were detected in all other areas of interest, except the target AOI in relation to the total samples.</p>
<p><span class="math display">\[
TR = \frac{\text{Samples on target AOI}}{\text{Total samples}}
\]</span> <span class="math display">\[
NTR = \frac{\text{Samples on non-target AOIs}}{\text{Total samples}}
\]</span></p>
<p>With these two introduced metrics, a variety of interesting analyses and visualisations is possible. The target (TR) and non-target (NTR) ratio can be looked at for an individual subject and sentence with a specific condition (see <a href="#fig-trntrstatic" class="quarto-xref">Figure&nbsp;10</a>).</p>
<div id="fig-trntrstatic" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trntrstatic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig4_4_9.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trntrstatic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Example targt ratio and non-target ratio for the static condition
</figcaption>
</figure>
</div>
<p>Another interesting insight is the average TR and NTR over all subjects for one sentence (see <a href="#fig-trntrsentence" class="quarto-xref">Figure&nbsp;11</a>).</p>
<div id="fig-trntrsentence" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trntrsentence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig4_4_8.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trntrsentence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Example TR and NTR for an example sentence over all subjects
</figcaption>
</figure>
</div>
<p>Our goal is to compare the target ratio and non-target ratio for sentences with the static condition to the sentences with the motion condition. The TR and NTR are measured exactly 50 milliseconds before the <em>TARGETONSET</em> event and thereby before the target word is said. Since its only one measure point the TR and NTR for a single sentence can be 1 or 0. For each subject, the average target and non-target ratio for all sentences with the static condition <em>tr(static)</em> and motion condition <em>tr(motion)</em> is calculated. <br></p>
<p>With the resulting metrics for each subject, the total average, mean and standard deviation are calculated, visualized in a boxplot and interpreted. <br></p>
<p>The scripts to calculate all metrics mentioned above can be found in the project folder under <em>~/scripts/</em>.</p>
</section>
</section>
<section id="sec-results" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Results</h1>
<p>In the following, we present the results of our conducted experiment. We collected mainly quantitative data by recording the gaze of our participants, but we extended it with qualitative data by keeping lab notes for each participant to have more context for salient gaze behavior.</p>
<section id="participants" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="participants"><span class="header-section-number">4.1</span> Participants</h2>
<p>We had 10 people participating in our experiment between the ages 18 and 34 where nine people identified as male and one as female. Regarding their English level, most (6) people specified that they have reached at least a C1 level and the remaining (4) people were at a B2 level. Additionally, we asked the participants about their previous experience with eye-tracking. For the majority (7) it was the first time using eye-trackers, the other three stated that they have used eye-trackers before.</p>
<p>Overall, all the participants were able to complete the experiment successfully without significant complications.</p>
</section>
<section id="pilot-data" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="pilot-data"><span class="header-section-number">4.2</span> Pilot Data</h2>
<p>The initial data visualizations with the pilot data gave us a first impression of the data we were about to collect and provided us a semantic sanity check, whether our experiment design was actually suitable for the research question we had in mind. The visualizations additionally gave us valuable feedback on the technical implementation of the experiment. Through the visualization, we decided to remove the fixation dot in the middle of the screen when the stimuli are visible, since it inadverently directed the participant’s gaze to some extent. <a href="#fig-pilot" class="quarto-xref">Figure&nbsp;12</a> shows the visualizations for one trial from our pilot study.</p>
<div id="fig-pilot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pilot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/Fig9_8_7.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pilot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Visualized pilot data including sub-stimuli with fixdot
</figcaption>
</figure>
</div>
</section>
<section id="gaze-on-target-behavior" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="gaze-on-target-behavior"><span class="header-section-number">4.3</span> Gaze on Target Behavior</h2>
<p><a href="#fig-boxplot-global" class="quarto-xref">Figure&nbsp;13</a> illustrates the boxplots for the mean TR and NTR as well as the static and motion condition for all participants. Overall, the interquartile range (IQR) of TR for the motion condition is slightly higher than for the static. The static condition is shifted by around 0.2 towards higher values, although the TR-median is 0.1 higher for the motion condition. The values of NTR behave in an opposite way. This indicates that the IQR is higher for the static condition and shifted by 0.2 towards lower values. The NTR share for both conditions the same median. Additionally, metrics that are not directly visible in the boxplot, as well as supplementary metrics, can be found in <a href="#tbl-condition-group-comparison" class="quarto-xref">Table&nbsp;3</a>. The values from the table emphasize the insights from the boxplot and demonstrate that the TR is marginally higher for the static condition than the motion condition. <br></p>
<div id="fig-boxplot-global" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boxplot-global-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/4-analysis-boxplot-tr-and-ntr.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boxplot-global-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Boxplot diagrams of our main metrics for both static and motion conditions
</figcaption>
</figure>
</div>
<div id="tbl-condition-group-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-condition-group-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Metric comparison between the participant groups and the complete data set (global)
</figcaption>
<div aria-describedby="tbl-condition-group-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Global</th>
<th style="text-align: center;">Wolf</th>
<th style="text-align: center;">Chicken</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Mean TR (static)</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.562</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mean TR (motion)</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.519</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mean NTR (static)</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.356</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mean NTR (motion)</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.431</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Stdev TR (static)</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.225</td>
</tr>
<tr class="even">
<td style="text-align: left;">Stdev TR (motion)</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.185</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Stdev NTR (static)</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.111</td>
</tr>
<tr class="even">
<td style="text-align: left;">Stdev NTR (motion)</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.166</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Furthermore, we split the analysis up to our two participant groups to check, if we obtain noticeable differences in the data. The data is listed in <a href="#tbl-condition-group-comparison" class="quarto-xref">Table&nbsp;3</a> and visualized as boxplots in <a href="#fig-boxplot-chicken" class="quarto-xref">Figure&nbsp;14</a> and <a href="#fig-boxplot-wolf" class="quarto-xref">Figure&nbsp;15</a>.</p>
<div id="fig-boxplot-chicken" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boxplot-chicken-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/4-analysis-boxplot-chicken-tr-and-ntr.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boxplot-chicken-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Boxplot diagrams of our main metrics for both static and motion conditions for the participant group chicken
</figcaption>
</figure>
</div>
<div id="fig-boxplot-wolf" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boxplot-wolf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Images/4-analysis-boxplot-wolf-tr-and-ntr.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boxplot-wolf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Boxplot diagrams of our main metrics for both static and motion conditions group wolf
</figcaption>
</figure>
</div>
<p>The results indicate, that there is a larger standard deviation in the static condition in group wolf compared to group chicken. However, we can still identify the same trend that we saw in the global analysis, which is that the static condition performs slightly better (higher target rate) than the motion condition. Another thing to note here is that we only have five participants in each group, meaning that these statistical analyses are more sensitive to outliers.</p>
</section>
</section>
<section id="sec-discussion" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Discussion</h1>
<p>Our results show that our participants did look at related objects in the visual stimulus before hearing it in a sentence, by inferring from previous parts of the sentence, such as the verb or the real-world context they appear in. This anticipation effect was more visible in the static condition data, which was set up similarly to previous work by Altmann &amp; Kamide <span class="citation" data-cites="altmann_incremental_1999">(<a href="#ref-altmann_incremental_1999" role="doc-biblioref">1999</a>)</span>. The second motion condition, although showing lower results compared to the static one, indicates that the participants still made anticipatory eye movements before hearing the target word at the end of the sentence. The added movement in the stimuli might be a distracting factor which lowered the anticipation effect, but due to our small effect size, we cannot claim this assumption with high confidence. It is important to note that we have a very limited data set and we would need more participant data to be able to do meaningful significance tests to properly test our hypothesis. <br></p>
<p>Besides our small participant number, we have a list of other limitations to discuss. One limitation is, that unlike the previously referenced work, we did not conduct the study with native speakers of the English language. Altering the stimuli to be in German would introduce a series of new questions we would need to ask ourselves regarding the stimuli design, and due to the limited scope of this course, we decided to keep the study design as simple as possible and orient it to our previously referenced work.</p>
<p>Adding to our list of limitations, we did not collect much qualitative data in our study. Including open-ended questions in our questionnaire at the end might have helped to put the gaze data of our participants more into context and potentially collect feedback on the experiment design overall.</p>
<p>We observed some of our participants getting bored during the experiment, either by telling us or they started fidgeting more during the trials and seemed impatient. This indicates that we should plan a mandatory short break halfway through the trials since the participants might not feel the need to speak up about needing a short break. The limitation we see here is that the lack of breaks might have led to negative effects on the quality of our collected data.</p>
<p>Further problems we encountered during the project were issues with the eye-tracker setup. With some participants, the calibration process worked right away without any issues, with others their earlobes were sometimes detected as eyes and interfered with our gaze recordings. Another problem as mentioned in <a href="#sec-impl" class="quarto-xref">Section&nbsp;2.4</a>, there is no direct way to incorporate videos in OpenSesame, which forced us to come up with our own solution to include moving stimuli.</p>
<p>Finally, handling the stimuli turned out to be more difficult than expected. With 64 sentences and 4 images for each condition, we spent a long time finding fitting images and mapping them correctly to each sentence in a way that there is only one obvious fitting image to the spoken sentence for each trial. Having a central document for writing down all the sentences and their corresponding image files helped to not lose sight. Finding fitting GIFs was a bigger challenge than for static images since the loop length is different for each GIF. During the implementation, we had to swap out several GIFs we picked out previously to somewhat match the loop length of the four sub-stimuli.</p>
<section id="conclusion-outlook" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="conclusion-outlook"><span class="header-section-number">5.1</span> Conclusion &amp; Outlook</h2>
<p>In conclusion, we designed and conducted an eye-tracking experiment which implemented the Visual World Paradigm and we were interested in the predictive gaze behavior of participants, when presented with audio and related visual stimuli. Our results show similar behavior as shown in previous work, which is that humans were looking at visual images that were related to the sentence they were hearing, even before actually hearing the word of the object they were looking at. We were interested in whether this “anticipation effect” was also visible when participants were presented with video stimuli instead of static images. Our results show a slightly better target match ratio with static stimuli, compared to moving stimuli. However, our sample size with N=10 is small and the experiment needs to be conducted with more participants to have a more stable data basis for significance tests.</p>
<p>The project enriched us with several lessons on eye-tracking, implementing experiments and conducting them with human participants. Especially conducting experiments with humans showed us that every human is individual and the study setup needs to be adjusted to their needs to be completed successfully e.g.&nbsp;adjusting the eye-tracker angle for each participant and needing to re-adjust it in case of insufficient calibration. But also the need to consider that some people like to proceed quickly through all the trials, while others like to take their time. Another lesson we have learned is that not all eye-tracking data is to be treated equally and varies depending on the type of eye-tracker used but also on other factors such as light conditions in the room and even whether the tracked person is wearing visual aids. During the design and implementation of the experience, we learned that we underestimated the task of creating and managing the stimuli used in the experiment. Among the things we needed to consider during the stimuli creation process were aspects such as ensuring syntactic similarity between the sentences used in the audio stimuli and finding fitting images (both static and GIFs) for target words and non-related distractors. The project also showcased to us the importance of conducting a pilot study to improve any oversights in the implementation, before inviting further participants. Since this experiment was conducted as part of a university course, we had to limit the scope of this project to fit our time and human resource constraints. Nevertheless, we propose some related key topics that came up during this project that would be interesting to be investigated in the future: <br></p>
<p style="text-align: left;">
<strong><em>How would the results look like in more “lab-like” conditions?</em></strong>
</p>
<p>This includes a more uniform stimuli design with self-created stimuli with the same frame rate and color palettes and only inviting participants that are native speakers of the language used in the study.</p>
<p style="text-align: left;">
<strong><em>Do people still make predictive eye movements when the target image is static and the distractors are moving?</em></strong>
</p>
<p>This could show whether the anticipation effect phenomenon would get overpowered by the saliency of visual motion.</p>
<p style="text-align: left;">
<strong><em>How about when the target is moving and the distractors are static?</em></strong>
</p>
<p>The experiment introduces similar questions for different combinations of static/non-static and target/distractors and the potential use of filler stimuli.</p>
<p>All in all, there are several open research questions regarding the visual world paradigm and the use of moving stimuli that need further investigation.</p>
</section>
</section>
<section id="contribution-table" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Contribution Table</h1>
<p>The following table gives a detailed insight on how the workload of this project was split between the group members:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Task</th>
<th>Yanhong</th>
<th>Dilara</th>
<th>Karl Jorge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Background Literature</td>
<td>x</td>
<td>o</td>
<td></td>
</tr>
<tr class="even">
<td>Experiment Design</td>
<td></td>
<td>x</td>
<td></td>
</tr>
<tr class="odd">
<td>Experiment Implementation</td>
<td></td>
<td></td>
<td>x</td>
</tr>
<tr class="even">
<td>Stimulus and audio design</td>
<td>x</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Piloting</td>
<td></td>
<td>x</td>
<td>x</td>
</tr>
<tr class="even">
<td>Pilot plots</td>
<td></td>
<td></td>
<td>x</td>
</tr>
<tr class="odd">
<td>Data-Recording</td>
<td>o</td>
<td>x</td>
<td>x</td>
</tr>
<tr class="even">
<td>Data Analysis Scripts</td>
<td></td>
<td></td>
<td>x</td>
</tr>
<tr class="odd">
<td>Report Writing</td>
<td>o</td>
<td>x</td>
<td>o</td>
</tr>
<tr class="even">
<td>Report Lectorate</td>
<td>x</td>
<td>x</td>
<td>x</td>
</tr>
<tr class="odd">
<td>Participant Acquisition</td>
<td>x</td>
<td>x</td>
<td>x</td>
</tr>
<tr class="even">
<td>Project Management</td>
<td></td>
<td>x</td>
<td></td>
</tr>
</tbody>
</table>
<p>Legend: x = main, o = supporter</p>

</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-abrams_motion_2003" class="csl-entry" role="listitem">
Abrams, Richard A., and Shawn E. Christ. 2003. <span>“Motion <span>Onset</span> <span>Captures</span> <span>Attention</span>.”</span> <em>Psychol Sci</em> 14 (5): 427–32. <a href="https://doi.org/10.1111/1467-9280.01458">https://doi.org/10.1111/1467-9280.01458</a>.
</div>
<div id="ref-allopenna_tracking_1998" class="csl-entry" role="listitem">
Allopenna, Paul D., James S. Magnuson, and Michael K. Tanenhaus. 1998. <span>“Tracking the <span>Time</span> <span>Course</span> of <span>Spoken</span> <span>Word</span> <span>Recognition</span> <span>Using</span> <span>Eye</span> <span>Movements</span>: <span>Evidence</span> for <span>Continuous</span> <span>Mapping</span> <span>Models</span>.”</span> <em>Journal of Memory and Language</em> 38 (4): 419–39. <a href="https://doi.org/10.1006/jmla.1997.2558">https://doi.org/10.1006/jmla.1997.2558</a>.
</div>
<div id="ref-altmann_thematic_1999" class="csl-entry" role="listitem">
Altmann, Gerry T. M. 1999. <span>“Thematic <span>Role</span> <span>Assignment</span> in <span>Context</span>.”</span> <em>Journal of Memory and Language</em> 41 (1): 124–45. <a href="https://doi.org/10.1006/jmla.1999.2640">https://doi.org/10.1006/jmla.1999.2640</a>.
</div>
<div id="ref-altmann_incremental_1999" class="csl-entry" role="listitem">
Altmann, Gerry T. M, and Yuki Kamide. 1999. <span>“Incremental Interpretation at Verbs: Restricting the Domain of Subsequent Reference.”</span> <em>Cognition</em> 73 (3): 247–64. <a href="https://doi.org/10.1016/S0010-0277(99)00059-1">https://doi.org/10.1016/S0010-0277(99)00059-1</a>.
</div>
<div id="ref-schmid_eye-tracking_2016" class="csl-entry" role="listitem">
Berends, Sanne M., Susanne M. Brouwer, and Simone A. Sprenger. 2016. <span>“Eye-<span>Tracking</span> and the <span>Visual</span> <span>World</span> <span>Paradigm</span>.”</span> In <em>Designing <span>Research</span> on <span>Bilingual</span> <span>Development</span></em>, 55–80. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-11529-0_5">https://doi.org/10.1007/978-3-319-11529-0_5</a>.
</div>
<div id="ref-cooper_control_1974" class="csl-entry" role="listitem">
Cooper, Roger M. 1974. <span>“The Control of Eye Fixation by the Meaning of Spoken Language: <span>A</span> New Methodology for the Real-Time Investigation of Speech Perception, Memory, and Language Processing.”</span> <em>Cognitive Psychology</em> 6 (1): 84–107. <a href="https://doi.org/10.1016/0010-0285(74)90005-X">https://doi.org/10.1016/0010-0285(74)90005-X</a>.
</div>
<div id="ref-elgort_cross-language_2023" class="csl-entry" role="listitem">
Elgort, Irina, Marc Brysbaert, and Anna Siyanova-Chanturia. 2023. <span>“Cross-Language <span>Influences</span> in <span>Bilingual</span> <span>Processing</span> and <span>Second</span> <span>Language</span> <span>Acquisition</span>,”</span> 1–327. <a href="https://doi.org/10.1075/bpa.16">https://doi.org/10.1075/bpa.16</a>.
</div>
<div id="ref-huettig_using_2011" class="csl-entry" role="listitem">
Huettig, Falk, Joost Rommers, and Antje S. Meyer. 2011. <span>“Using the Visual World Paradigm to Study Language Processing: <span>A</span> Review and Critical Evaluation.”</span> <em>Acta Psychologica</em>, Visual search and visual world: <span>Interactions</span> among visual attention, language, and working memory, 137 (2): 151–71. <a href="https://doi.org/10.1016/j.actpsy.2010.11.003">https://doi.org/10.1016/j.actpsy.2010.11.003</a>.
</div>
<div id="ref-spencer2016stereotype" class="csl-entry" role="listitem">
Spencer, Steven J, Christine Logel, and Paul G Davies. 2016. <span>“Stereotype Threat.”</span> <em>Annual Review of Psychology</em> 67: 415–37. <a href="https://doi.org/10.1111/j.1559-1816.2008.00362.x">https://doi.org/10.1111/j.1559-1816.2008.00362.x</a>.
</div>
<div id="ref-tanenhaus_integration_1995" class="csl-entry" role="listitem">
Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C. Sedivy. 1995. <span>“Integration of <span>Visual</span> and <span>Linguistic</span> <span>Information</span> in <span>Spoken</span> <span>Language</span> <span>Comprehension</span>.”</span> <em>Science</em> 268 (5217): 1632–34. <a href="https://doi.org/10.1126/science.7777863">https://doi.org/10.1126/science.7777863</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>https://www.acoust.io/<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://osdoc.cogsci.nl/3.3/tutorials/visual-world/<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Can be found in the project folder under ~/stimuli/0-stimuli-table.xlsx<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://pypi.org/project/PyYAML/<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>https://pypi.org/project/opencv-python/<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>https://www.python.org/downloads/release/python-3124/<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>https://pypi.org/project/pandas/<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>https://pypi.org/project/numpy/<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>https://pypi.org/project/matplotlib/<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>https://pypi.org/project/seaborn/<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>https://www.gazept.com/dl/Gazepoint_API_v2.0.pdf<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>https://pypi.org/project/scipy/<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Can be found in the project folder under ~/stimuli/0-stimuli-table.xlsx<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>