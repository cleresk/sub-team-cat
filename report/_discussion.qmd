Our results show that our participants did look at related objects in the visual stimulus before hearing it in a sentence, by inferring from previous parts of the sentence, such as the verb or the real-world context they appear in. This anticipation effect was more visible in the static condition data, which was set up similarly to previous work by Altmann & Kamide [-@altmann_incremental_1999]. The second motion condition, although showing lower results compared to the static one, indicates that the participants still made anticipatory eye movements before hearing the target word at the end of the sentence. The added movement in the stimuli might be a distracting factor which lowered the anticipation effect, but due to our small effect size, we cannot claim this assumption with high confidence.
It is important to note that we have a very limited data set and we would need more participant data to be able to do meaningful significance tests to properly test our hypothesis. </br>

Besides our small participant number, we have a list of other limitations to discuss. 
One limitation is, that unlike the previously referenced work, we did not conduct the study with native speakers of the English language. Altering the stimuli to be in German would introduce a series of new questions we would need to ask ourselves regarding the stimuli design, and due to the limited scope of this course, we decided to keep the study design as simple as possible and orient it to our previously referenced work.

Adding to our list of limitations, we did not collect much qualitative data in our study. Including open-ended questions in our questionnaire at the end might have helped to put the gaze data of our participants more into context and potentially collect feedback on the experiment design overall. 

We observed some of our participants getting bored during the experiment, either by telling us or they started fidgeting more during the trials and seemed impatient. This indicates that we should plan a mandatory short break halfway through the trials since the participants might not feel the need to speak up about needing a short break. The limitation we see here is that the lack of breaks might have led to negative effects on the quality of our collected data. 

Further problems we encountered during the project were issues with the eye-tracker setup. With some participants, the calibration process worked right away without any issues, with others their earlobes were sometimes detected as eyes and interfered with our gaze recordings. Another problem as mentioned in @sec-impl,  there is no direct way to incorporate videos in OpenSesame, which forced us to come up with our own solution to include moving stimuli. 

Finally, handling the stimuli turned out to be more difficult than expected. With 64 sentences and 4 images for each condition, we spent a long time finding fitting images and mapping them correctly to each sentence in a way that there is only one obvious fitting image to the spoken sentence for each trial. Having a central document for writing down all the sentences and their corresponding image files helped to not lose sight. Finding fitting GIFs was a bigger challenge than for static images since the loop length is different for each GIF. During the implementation, we had to swap out several GIFs we picked out previously to somewhat match the loop length of the four sub-stimuli.




## Conclusion & Outlook

In conclusion, we designed and conducted an eye-tracking experiment which implemented the Visual World Paradigm and we were interested in the predictive gaze behavior of participants, when presented with audio and related visual stimuli. Our results show similar behavior as shown in previous work, which is that humans were looking at visual images that were related to the sentence they were hearing, even before actually hearing the word of the object they were looking at. We were interested in whether this "anticipation effect" was also visible when participants were presented with video stimuli instead of static images. Our results show a slightly better target match ratio with static stimuli, compared to moving stimuli. However, our sample size with N=10 is small and the experiment needs to be conducted with more participants to have a more stable data basis for significance tests.


The project enriched us with several lessons on eye-tracking, implementing experiments and conducting them with human participants. Especially conducting experiments with humans showed us that every human is individual and the study setup needs to be adjusted to their needs to be completed successfully e.g. adjusting the eye-tracker angle for each participant and needing to re-adjust it in case of insufficient calibration. But also the need to consider that some people like to proceed quickly through all the trials, while others like to take their time. Another lesson we have learned is that not all eye-tracking data is to be treated equally and varies depending on the type of eye-tracker used but also on other factors such as light conditions in the room and even whether the tracked person is wearing visual aids. 
During the design and implementation of the experience, we learned that we underestimated the task of creating and managing the stimuli used in the experiment. Among the things we needed to consider during the stimuli creation process were aspects such as ensuring syntactic similarity between the sentences used in the audio stimuli and finding fitting images (both static and GIFs) for target words and non-related distractors. The project also showcased to us the importance of conducting a pilot study to improve any oversights in the implementation, before inviting further participants. 
Since this experiment was conducted as part of a university course, we had to limit the scope of this project to fit our time and human resource constraints. Nevertheless, we propose some related key topics that came up during this project that would be interesting to be investigated in the future: </br>


<p style="text-align: left;">***How would the results look like in more "lab-like" conditions?***</p>
This includes a more uniform stimuli design with self-created stimuli with the same frame rate and color palettes and only inviting participants that are native speakers of the language used in the study.

<p style="text-align: left;">***Do people still make predictive eye movements when the target image is static and the distractors are moving?***</p>
This could show whether the anticipation effect phenomenon would get overpowered by the saliency of visual motion. 

<p style="text-align: left;">***How about when the target is moving and the distractors are static?***</p>

The experiment introduces similar questions for different combinations of static/non-static and target/distractors and the potential use of filler stimuli.

All in all, there are several open research questions regarding the visual world paradigm and the use of moving stimuli that need further investigation. 










