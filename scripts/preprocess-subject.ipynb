{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables / settings for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject-independant directory and filename of raw-data\n",
    "input_subject_file = f\"..\\\\data\\\\raw\\\\subject-{subject_no}.tsv\"\n",
    "# sample rate of the eye tracker used to gather raw data\n",
    "sample_rate = 150\n",
    "# expected logs for one experiment (START_TRIAL {NUM}, FIXATION, PREVIEW, AUDIOSTART, VERBONSET, TARGETONSET, PAUSE)\n",
    "expected_logs = [64, 64, 64, 64, 64, 64, 0]\n",
    "# custom fields of log to be copied\n",
    "fields_to_copy = [\"GROUP\", \"SENTENCE_ID\", \"SENTENCE\", \"STIMULUS_NAME\", \"CONDITION\", \"AUDIO\", \"SSTIM_TL\", \"SSTIM_TR\", \"SSTIM_BL\", \"SSTIM_BR\", \"TARGET_POS\", \"LOCATION_CUE\", \"VERB_CUE_TIMING\", \"VERB_CUE\", \"TARGET_CUE_TIMING\", \"TRIAL\", \"SUBJECT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv(input_subject_file, sep=\"\\t\", header=0)\n",
    "\n",
    "# drop unnecessary data\n",
    "data = data.drop([\"TIME_TICK\"], axis=1)\n",
    "data = data.drop([\"FPOGX\", \"FPOGY\", \"FPOGS\", \"FPOGD\", \"FPOGID\", \"FPOGV\"], axis=1)\n",
    "data = data.drop([\"LPOGX\", \"LPOGY\", \"LPOGV\", \"RPOGX\", \"RPOGY\", \"RPOGV\"], axis=1)\n",
    "data = data.drop([\"LPCX\", \"LPCY\", \"LPD\", \"LPS\", \"LPV\", \"RPCX\", \"RPCY\", \"RPD\", \"RPS\", \"RPV\"], axis=1)\n",
    "data = data.drop([\"LEYEX\", \"LEYEY\", \"LEYEZ\", \"LPUPILD\", \"LPUPILV\", \"REYEX\", \"REYEY\", \"REYEZ\", \"RPUPILD\", \"RPUPILV\"], axis=1)\n",
    "data = data.drop([\"CX\", \"CY\", \"CS\"], axis=1)\n",
    "\n",
    "# display data\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create plots directory for subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder name\n",
    "plt_folder_name = f\"..\\\\plots\\\\subject-{subject_no}\"\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(plt_folder_name):\n",
    "    os.makedirs(plt_folder_name)\n",
    "    print(f\"Folder {plt_folder_name} created.\")\n",
    "else:\n",
    "    print(f\"Folder {plt_folder_name} already exists. Skipping...\")\n",
    "\n",
    "plots_folder_name = os.path.abspath(plt_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dtypes of dataframe\n",
    "Make sure, that columns have no mixed dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    print(column,\":\",pd.api.types.infer_dtype(data[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sanity check\n",
    "### 1.1 Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the occurencies of \"word\" in data\n",
    "def find_occur(data, word):\n",
    "    all_text = \" \".join(data[\"USER\"].astype(str).values.flatten())\n",
    "    total_count = all_text.count(word)\n",
    "    return total_count\n",
    "\n",
    "# add labels on top of the bars\n",
    "def add_bar_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(\"{}\".format(height),\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# index\n",
    "log_index = [\"START_TRIAL\", \"EVENT=FIXATION\", \"EVENT=PREVIEW\", \"EVENT=AUDIOSTART\", \"EVENT=PAUSE\", \"STOP_TRIAL\", \"TRIAL_WARNING\"]\n",
    "ind = np.arange(len(log_index))\n",
    "\n",
    "count = []\n",
    "for log in log_index:\n",
    "    count.append(find_occur(data, log))\n",
    "\n",
    "# width of the bars\n",
    "width = 0.25\n",
    "\n",
    "# barplot data\n",
    "logs = pd.DataFrame({\"count\": count, \"expected\": expected_logs}, index=log_index)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "count_bar = ax.bar(ind - width/2, count, width, label=\"count\", color=\"#b8c1f2\")\n",
    "add_bar_labels(count_bar)\n",
    "expected_bar = ax.bar(ind + width/2, expected_logs, width, label=\"expected\", color=\"#3a3f58\")\n",
    "add_bar_labels(expected_bar)\n",
    "\n",
    "# labels and ticks\n",
    "ax.set_yticks(np.arange(0, np.max(expected_logs)+15, step=8))\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(log_index)\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(f\"Sanity check logging for subject {subject_no}\")\n",
    "ax.legend()\n",
    "\n",
    "# display plot\n",
    "plt.show()\n",
    "\n",
    "# save plot to assets\n",
    "fig.savefig(f\"{plots_folder_name}/1-sanity-logging_events.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Time between samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate time between samples (delta) in milliseconds\n",
    "time_deltas = (data[\"TIME\"].diff() * 1000).fillna(0).astype(int)\n",
    "\n",
    "# count occurrences of each delta\n",
    "delta_counts = time_deltas.value_counts()\n",
    "\n",
    "mn = delta_counts.idxmax()\n",
    "mn_max = delta_counts.max()\n",
    "l_dev = mn - 3  \n",
    "r_dev = mn + 3\n",
    "\n",
    "# adjust bins as needed\n",
    "bins= np.arange(l_dev, r_dev, step=0.5)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.hist(time_deltas, width=0.5, bins=bins, color=\"#3a3f58\")\n",
    "\n",
    "# labels and ticks\n",
    "plt.title(f\"Histogram excerpt of time delta between eyetracker samples for subject {subject_no}\")\n",
    "plt.xlabel(\"Time delta in milliseconds\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlim([l_dev,r_dev])\n",
    "\n",
    "plt.ylim([0,mn_max+6000])\n",
    "plt.xticks(bins)\n",
    "\n",
    "# Calculate bin counts\n",
    "bin_counts, _ = np.histogram(time_deltas, bins=bins)\n",
    "\n",
    "# add annotations on top of bars\n",
    "for i in range(len(bins) - 1):\n",
    "    if(bin_counts[i] == 0):\n",
    "        continue\n",
    "    plt.text((bins[i] + bins[i+1]) / 2, bin_counts[i], str(bin_counts[i]), ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "\n",
    "# display plot\n",
    "plt.show()  \n",
    "\n",
    "# save plot to assets\n",
    "fig.savefig(f\"{plots_folder_name}/1-sanity-hist_time_deltas.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preporcessing and enhancement of data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Interpolation\n",
    "#### 2.1.1. Implement interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy original data, to compare later on with interpolated data\n",
    "before_interpol_data = data.copy()\n",
    "\n",
    "# interpolate BPOGX and BPOGY\n",
    "interp_func_x = interp1d(data[\"TIME\"], data[\"BPOGX\"], kind=\"linear\", fill_value=\"interpolate\")\n",
    "interp_func_y = interp1d(data[\"TIME\"], data[\"BPOGY\"], kind=\"linear\", fill_value=\"interpolate\")\n",
    "interp_func_v = interp1d(data[\"TIME\"], data[\"BPOGV\"], kind=\"linear\", fill_value=\"interpolate\")\n",
    "\n",
    "# generate new evenly spaced time points\n",
    "t_delta = data[\"TIME\"].max() - data[\"TIME\"].min() \n",
    "lin_num = (t_delta * sample_rate).round(0).astype(int)\n",
    "\n",
    "new_time = np.linspace(data[\"TIME\"].min(), data[\"TIME\"].max(), num=lin_num)\n",
    "\n",
    "# get interpolated values with new time\n",
    "new_bpogx = interp_func_x(new_time)\n",
    "new_bpogy = interp_func_y(new_time)\n",
    "new_bpogv = interp_func_v(new_time)\n",
    "\n",
    "# overwrite original dataframe and round like original data\n",
    "data = pd.DataFrame({})\n",
    "data[\"TIME\"] = new_time\n",
    "data[\"BPOGX\"] = new_bpogx.astype(float)\n",
    "data[\"BPOGY\"] = new_bpogy.astype(float)\n",
    "data[\"BPOGV\"] = new_bpogv.round(0).astype(int)\n",
    "\n",
    "def find_nearest_index(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "for index, row in before_interpol_data.iterrows():\n",
    "    if pd.notna(row[\"USER\"]):  # Check if USER event is present\n",
    "        nearest_index = find_nearest_index(new_time, row[\"TIME\"])\n",
    "        # Add or append the event to the USER column\n",
    "        data.at[nearest_index, \"USER\"] = row[\"USER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Visualize Interpolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of the dataframe\n",
    "# ! assuming there are min. 1000 samples\n",
    "start_time = data[\"TIME\"][random.randint(0,1000)] \n",
    "section = (start_time,start_time+0.1)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# mark interpolated time points\n",
    "avxl_index = False\n",
    "for index, row in data.iterrows():\n",
    "    time = row[\"TIME\"]\n",
    "    if time > section[0] and time < section[1]:\n",
    "        plt.axvline(x=time, color=\"black\", linestyle=\":\", linewidth=1, label=\"interp. sample times\" if not avxl_index else \"\")\n",
    "        avxl_index = True\n",
    "\n",
    "# BPOGX/Y interpol.\n",
    "plt.plot(data[\"TIME\"], data[\"BPOGX\"], \"-\", label=\"interp. BPOGX\", color=\"#b8c1f2\")\n",
    "plt.plot(data[\"TIME\"], data[\"BPOGY\"]-1, \"-\", label=\"interp. BPOGY\", color=\"orange\")\n",
    "\n",
    "# BPOGX samples\n",
    "plt.plot(before_interpol_data[\"TIME\"], before_interpol_data[\"BPOGX\"], \"o\", label=\"original BPOGX/BPOGY\", markersize=5, color=\"grey\")\n",
    "plt.plot(data[\"TIME\"], data[\"BPOGX\"], \"o\", label=\"interp. BPOGX samples\", markersize=5, color=\"#3a3f58\")\n",
    "\n",
    "# BPOGY samples\n",
    "plt.plot(before_interpol_data[\"TIME\"], before_interpol_data[\"BPOGY\"]-1, \"o\", markersize=5, color=\"grey\") # shift by 1 for visualisation\n",
    "plt.plot(data[\"TIME\"], data[\"BPOGY\"]-1, \"o\", label=\"interp. BPOGY samples\", markersize=5, color=\"#c45f0e\")\n",
    "\n",
    "# labels and ticks\n",
    "plt.title(f\"Excerpt of original vs interpolated BPOGX and BPOGY values over time for subject {subject_no}\")\n",
    "plt.xlabel(\"Time in seconds\")\n",
    "plt.ylabel(\"BPOGY / BPOGX\")\n",
    "plt.xlim((section[0],section[1]))\n",
    "plt.ylim((-1, 1))\n",
    "plt.legend(loc=\"center right\", framealpha=1)\n",
    "\n",
    "# display plot\n",
    "plt.show()\n",
    "\n",
    "# save plot to assets\n",
    "fig.savefig(f\"{plots_folder_name}/2-preprocessing-visualized_interpolation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. Verify interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc time between samples (deltas)\n",
    "time_deltas = (data[\"TIME\"].diff() * 1000).drop(0).astype(int)\n",
    "\n",
    "# count occurrences of each delta and print\n",
    "delta_counts = time_deltas.value_counts()\n",
    "print(delta_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Output should be a pandas Series with only one Value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Custom Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Decode custom logs and expand dataframe\n",
    "\n",
    "Input format for log message in USER-column:\n",
    "```\n",
    "VAR TRIAL_LOG <LOGSTING>\n",
    "VAR TRIAL_LOG EVENT=FIXATION;GROUP=WOLF;SENTENCE_ID=63;... # expample\n",
    "```\n",
    "Structure:\n",
    "```\n",
    "VAR TRIAL_LOG <KEY>=<VALUE>;<K>=<V>...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # add subject to every row\n",
    "    data.at[index, \"SUBJECT\"] = subject_no\n",
    "\n",
    "    # get loggin row of opengaze data\n",
    "    inp = str(row[\"USER\"])\n",
    "    \n",
    "    if inp.find(\"TRIAL_LOG\") != -1:\n",
    "        \n",
    "        cleaned_inp = inp.split(\"VAR TRIAL_LOG \", maxsplit=2)[1]\n",
    "        kv_inp = cleaned_inp.split(\";\")\n",
    "\n",
    "        # decode key/value-pairs and copy them to column (key) and value (cell)\n",
    "        for elem in kv_inp:\n",
    "\n",
    "            key, val = elem.split(\"=\", maxsplit=1)\n",
    "\n",
    "            data.at[index, key] = val\n",
    "    \n",
    "    # other custom logs\n",
    "    elif inp.find(\"TRIAL_WARNING\") != -1:\n",
    "        \n",
    "        data.at[index, \"EVENT\"] = \"WARNING\"\n",
    "        \n",
    "    elif inp.find(\"START_TRIAL\") != -1:\n",
    "        \n",
    "        data.at[index, \"EVENT\"] = \"START_TRIAL\"\n",
    "        data.at[index, \"TRIAL\"] = int(inp[11:].replace(\" \", \"\"))\n",
    "    \n",
    "    elif inp.find(\"STOP_TRIAL\") != -1:\n",
    "        \n",
    "        data.at[index, \"EVENT\"] = \"STOP_TRIAL\"\n",
    "        data.at[index, \"TRIAL\"] = int(inp[11:].replace(\" \", \"\"))\n",
    "\n",
    "# drop original log column\n",
    "data.drop(columns=[\"USER\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Calculate VERB- and TARGETONSET and add it to dataframe with corresponding log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend logs with the corresponding events\n",
    "audio_start_data = data.query(\"EVENT=='AUDIOSTART'\")\n",
    "print(len(audio_start_data))\n",
    "for index, row in audio_start_data.iterrows():\n",
    "    \n",
    "    sample_time = float(row[\"TIME\"])\n",
    "    \n",
    "    delta_verb_onset = float(row[\"VERB_CUE_TIMING\"])/1000\n",
    "    delta_target_onset = float(row[\"TARGET_CUE_TIMING\"])/1000\n",
    "    \n",
    "    time_verb_onset = sample_time + delta_verb_onset\n",
    "    time_target_onset = sample_time + delta_target_onset\n",
    "    \n",
    "    first_sample_verb = data[data[\"TIME\"] >= time_verb_onset].iloc[0].name\n",
    "    first_sample_target = data[data[\"TIME\"] >= time_target_onset].iloc[0].name\n",
    "    \n",
    "    for field in fields_to_copy:\n",
    "        data.at[first_sample_verb, field] = row[field]\n",
    "        data.at[first_sample_target, field] = row[field]\n",
    "    \n",
    "    # Set the EVENT to \"VERBONSET\"\n",
    "    data.at[first_sample_verb, \"EVENT\"] = \"VERBONSET\"\n",
    "    data.at[first_sample_target, \"EVENT\"] = \"TARGETONSET\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Expand empty cells in dataframe with corresponding log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure event gets copyied as well\n",
    "if \"EVENT\" not in fields_to_copy:\n",
    "    fields_to_copy.append(\"EVENT\")\n",
    "\n",
    "found_first_evt = False\n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # skip first samples with NaN\n",
    "    if \"START_TRIAL\" in str(data.at[index, \"EVENT\"]):\n",
    "        found_first_evt = True\n",
    "        continue\n",
    "\n",
    "    if not found_first_evt:\n",
    "        continue\n",
    "    \n",
    "    # copy last log if custom log cells are NaN\n",
    "    if pd.isna(data.at[index, \"EVENT\"]):\n",
    "        \n",
    "        for field in fields_to_copy:\n",
    "            data.at[index, field] = data.at[index-1, field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    print(column,\":\",pd.api.types.infer_dtype(data[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Add specific format to df rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"SUBJECT\"] = data[\"SUBJECT\"].fillna(0).astype(int)\n",
    "data[\"TRIAL\"] = data[\"TRIAL\"].fillna(0).astype(int)\n",
    "data[\"SENTENCE_ID\"] = data[\"SENTENCE_ID\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check final dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    print(f\"{column}: {pd.api.types.infer_dtype(data[column])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export Data to TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_subject_file = f\"..\\\\data\\\\preprocessed\\\\subject-{subject_no}.tsv\"\n",
    "data.to_csv(output_subject_file, sep=\"\\t\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
